<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Intro to R for Social Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Merlin Schaeffer &amp; Friedolin Merhout   Department of Sociology" />
    <meta name="date" content="2022-10-08" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="../template/Merlin169.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Intro to R for Social Data Science
]
.subtitle[
## 7 Bivariate OLS Regression
]
.author[
### Merlin Schaeffer &amp; Friedolin Merhout <br> Department of Sociology
]
.date[
### 2022-10-08
]

---


# Let's get ready


```r
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(ggplot2) # Allows us to create nice figures.
*library(estimatr) # Allows us to estimate (cluster-)robust standard errors.
*library(texreg) # Allows us to make nicely-formatted Html &amp; Latex regression tables.
*library(broom) # Allows us to turn model objects into tibbles.
```



---
class: clear


```r
ESS &lt;- ESS %&gt;% transmute( # Create new variables and keep only those
    cntry = as_factor(cntry) %&gt;% fct_drop(), # Country of interview
    gndr = as_factor(gndr),
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = dweight*pweight,
*   # Parental education (ISCED)
*   eiscedf = zap_labels(eiscedf),
*   eiscedm = zap_labels(eiscedm),
*   par_edu = case_when(
*     # If father's edu is missing but not mother's, take mother's
*     (eiscedf == 55 | is.na(eiscedf)) &amp; eiscedm != 55 &amp; !is.na(eiscedm) ~ eiscedm,
*     # If mother's edu is missing but not father's, take father's
*     (eiscedm == 55 | is.na(eiscedm)) &amp; eiscedf != 55 &amp; !is.na(eiscedf)  ~ eiscedf,
*     # If both are missing, its missing
*     eiscedf == 55 &amp; eiscedm == 55 ~ as.numeric(NA),
*     # For all others, take the average of both parents
*     TRUE ~ (eiscedm + eiscedf) / 2),
    eduyrs = case_when( # Education
      eduyrs &gt; 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs &lt; 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %&gt;% # Make it numeric, then
  dplyr::filter(
    # Keep only respondents with native-born parents,
    facntr == "Yes" &amp; mocntr == "Yes")
```

---
# Design matrix


```r
# Design matrix (i.e., tibble of the vars going into out model)
ESS &lt;- ESS %&gt;%
  select(cntry, imwbcnt, eduyrs, par_edu, gndr, pspwght) %&gt;%
  drop_na() # Casewise deletion.
```

---
# Modeling xenophobia

.push-left[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="7-Bivariate-OLS_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
.panel[.panel-name[Code]

```r
# Plot the distribution of imwbcnt
ggplot(data = ESS, aes(x = imwbcnt)) +
  geom_freqpoly() +
  theme_minimal()
```
]
]]]

--

.push-right[
#### Modeling
Coming up with a model of imwbcntphobia means: Can imwbcntphobia (i.e., `\(y\)`) be expressed as a function (i.e., `\(f()\)`) of another variable (i.e., `\(x\)`)?

`$$\hat{y} = f(x)$$`
The two critical questions are thus:

1. What is `\(f()\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]
]

---
# A linear Model of `\(y\)` .font60[Simple, but extremely versatile!]

.right-column[
`$$\hat{y} = \alpha + \beta x$$`

&lt;img src="./img/LinearModel.png" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.left-column[
Now, the two critical questions have turned to:
1. What are `\(\alpha\)` and `\(\beta\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]

]

---
# A linear Model of `\(y\)`

.left-column[
Let's try `\(x = \text{years of education}\)`, since schools in most countries teach humanism and tolerance: Is imwbcntphobia a linear (i.e., constant slope) function of one's years of education?
]

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="7-Bivariate-OLS_files/figure-html/unnamed-chunk-8-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, mapping = aes(y = imwbcnt, x = eduyrs)) +
  geom_jitter(aes(size = pspwght), alpha = 1/5) + 
  geom_smooth(aes(weight = pspwght), se = FALSE) +
  geom_smooth(aes(weight = pspwght), method = "lm", 
              se = FALSE, color = "red") +
  labs(y = "Xenophobia", x =  "Years of education", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Ordinary Least Squares

.left-column[
We find `\(\alpha\)` and `\(\beta\)` by choosing the one line that minimizes the *residual sum of squares*:

`$$\begin{align*}
      \min \text{RSS} &amp;= \min \sum_{i=1}^{n} e_{i}^{2} \\
      &amp;= \min \sum_{i=1}^{n} y_{i} - \hat{y_{i}} \\
      &amp;= \min \sum_{i=1}^{n} (y_{i} - (\alpha + \beta x_{i})^{2}
    \end{align*}$$`
    
.backgrnote[
Note, how the residuals are defined only in terms of `\(y\)` and thereby differ fundamentally from those that PCA minimizes!
]
]

.right-column[
&lt;img src="7-Bivariate-OLS_files/figure-html/OLS-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---
# The `lm()` function 

.right-column[

```r
# Estimate a linear OLS model and assign the results to object ols.
ols &lt;- lm(formula = imwbcnt ~ eduyrs, data = ESS)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = imwbcnt ~ eduyrs, data = ESS)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -5.913 -1.513 -0.245  1.487  5.690 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)   7.1154     0.0901    79.0   &lt;2e-16 ***
# eduyrs       -0.1336     0.0066   -20.2   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 2.4 on 14574 degrees of freedom
# Multiple R-squared:  0.0273,	Adjusted R-squared:  0.0272 
# F-statistic:  409 on 1 and 14574 DF,  p-value: &lt;2e-16
```
]

.left-column[

$$
\operatorname{\widehat{imwbcnt}} = 7.12 - 0.13(\operatorname{eduyrs})
$$


is the line that `\(\min \sum_{i=1}^{n} e_{i}^{2}\)`.

]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use (post-stratification) weights. 

.alert[But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values, and confidence intervals) will be wrong, because weights introduce heteroscedasticity.]
]

.right-column[

```r
# Estimate a weighted linear OLS model.
ols &lt;- lm(formula = imwbcnt ~ eduyrs, data = ESS, 
*         weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted Residuals:
#    Min     1Q Median     3Q    Max 
# -9.922 -0.719 -0.077  0.803 10.065 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  7.27426    0.08335    87.3   &lt;2e-16 ***
# eduyrs      -0.15275    0.00604   -25.3   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 1.59 on 14574 degrees of freedom
# Multiple R-squared:  0.0421,	Adjusted R-squared:  0.042 
# F-statistic:  640 on 1 and 14574 DF,  p-value: &lt;2e-16
```
]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use post-stratification weights.

But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values) will be wrong, because weights introduce heteroscedasticity.

.alert[We thus need to estimate heteroscedasticity-robust standard errors.] One comfortable possibility to do that is `estimatr::lm_robust()`.
]

.right-column[

```r
# Estimate a weighted linear OLS model.
*ols &lt;- lm_robust(formula = imwbcnt ~ eduyrs, data = ESS,
                 weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper    DF
# (Intercept)    7.274     0.1473    49.4 0.00e+00    6.986    7.563 14574
# eduyrs        -0.153     0.0107   -14.3 4.09e-46   -0.174   -0.132 14574
# 
# Multiple R-squared:  0.0421 ,	Adjusted R-squared:  0.042 
# F-statistic:  205 on 1 and 14574 DF,  p-value: &lt;2e-16
```
]

---
# Multiple linear OLS

.left-column[
&lt;img src="7-Bivariate-OLS_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;

.alert[But is parental education the only confounder?] 

.backgrnote[
A confounder is a common cause of both one's predictor `\(x_{i1}\)` (e.g., years of education) and of the outcome `\(y_i\)` (e.g., imwbcntphobia). Counfounder bias is: `\(\mathbb{E}(\tilde{\beta_{x_{1}}}) = \beta_{x_{1}} + \color{orange}{\beta_{x_{2}} \frac{\text{Cov}(x_{i1},x_{i2})}{\text{Var}(x_{i1})}}.\)`
]
]

.right-column[
In the social sciences we use OLS to test theories about causal effects. It is thus important to rule out alternative explanations.

```r
# Estimate a weighted linear OLS model.
ols2 &lt;- lm_robust(
* formula = imwbcnt ~ eduyrs + par_edu,
  data = ESS, weights = pspwght)
summary(ols2) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs + par_edu, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper    DF
# (Intercept)   7.2706     0.1470   49.46 0.00e+00    6.982   7.5588 14573
# eduyrs       -0.1353     0.0120  -11.31 1.48e-29   -0.159  -0.1119 14573
# par_edu      -0.0769     0.0218   -3.52 4.32e-04   -0.120  -0.0341 14573
# 
# Multiple R-squared:  0.0447 ,	Adjusted R-squared:  0.0445 
# F-statistic:  112 on 2 and 14573 DF,  p-value: &lt;2e-16
```
]

---
# Categorical predictors

.left-column[
.center[.content-box-blue[
How can we make sense of a categorical predictor in a linear model?]]
]

--

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="7-Bivariate-OLS_files/figure-html/dummies-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = case_when(
                       cntry == "Denmark" ~ 0,
                       cntry == "Sweden" ~ 1,
                       TRUE ~ as.numeric(NA)
                     ))) +
  geom_jitter(aes(size = pspwght), alpha = 1/15, width = 0.05, height = 0) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(0, 1), 
                     labels = c("0 = Denmark", "1 = Sweden")) +
  labs(x = "Country", y = "Xenophobia", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Categorical predictors

.left-column[
Factors are automatically treated as dummies, with the first level taken as reference. Thus if you want to change the reference, you need to recode the levels of the factor using the `forcats` package.
]

.right-column[
.font90[

```r
ols3 &lt;- lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, 
                  data = ESS, weights = pspwght)
summary(ols3) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, 
#     data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#                Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper    DF
# (Intercept)      7.1324     0.1525  46.755 0.00e+00    6.833   7.4314 14563
# eduyrs          -0.0975     0.0124  -7.886 3.35e-15   -0.122  -0.0732 14563
# par_edu         -0.1628     0.0223  -7.314 2.72e-13   -0.206  -0.1192 14563
# gndrFemale      -0.1475     0.0624  -2.361 1.82e-02   -0.270  -0.0250 14563
# cntryCzechia     1.0558     0.0792  13.337 2.43e-40    0.901   1.2110 14563
# cntryEstonia     0.0371     0.0833   0.446 6.56e-01   -0.126   0.2005 14563
# cntryFinland    -1.0057     0.0745 -13.493 3.06e-41   -1.152  -0.8596 14563
# cntryFrance     -0.3276     0.0769  -4.260 2.06e-05   -0.478  -0.1769 14563
# cntryCroatia    -0.4285     0.0945  -4.533 5.87e-06   -0.614  -0.2432 14563
# cntryHungary     0.5585     0.0713   7.835 5.02e-15    0.419   0.6982 14563
# cntryLithuania  -0.3525     0.0906  -3.892 9.99e-05   -0.530  -0.1750 14563
# cntrySlovenia    0.0316     0.0864   0.366 7.15e-01   -0.138   0.2010 14563
# cntrySlovakia    1.0010     0.0905  11.058 2.59e-28    0.824   1.1784 14563
# 
# Multiple R-squared:  0.102 ,	Adjusted R-squared:  0.102 
# F-statistic:  116 on 12 and 14563 DF,  p-value: &lt;2e-16
```
]]

---
# Regression tables

.font90[

```r
# Regression table of model objects ols1 to ols3; report standard errors, not confidence intervals.
texreg::screenreg(list(ols, ols2, ols3), include.ci = FALSE) 
# 
# ========================================================
#                 Model 1       Model 2       Model 3     
# --------------------------------------------------------
# (Intercept)         7.27 ***      7.27 ***      7.13 ***
#                    (0.15)        (0.15)        (0.15)   
# eduyrs             -0.15 ***     -0.14 ***     -0.10 ***
#                    (0.01)        (0.01)        (0.01)   
# par_edu                          -0.08 ***     -0.16 ***
#                                  (0.02)        (0.02)   
# gndrFemale                                     -0.15 *  
#                                                (0.06)   
# cntryCzechia                                    1.06 ***
#                                                (0.08)   
# cntryEstonia                                    0.04    
#                                                (0.08)   
# cntryFinland                                   -1.01 ***
#                                                (0.07)   
# cntryFrance                                    -0.33 ***
#                                                (0.08)   
# cntryCroatia                                   -0.43 ***
#                                                (0.09)   
# cntryHungary                                    0.56 ***
#                                                (0.07)   
# cntryLithuania                                 -0.35 ***
#                                                (0.09)   
# cntrySlovenia                                   0.03    
#                                                (0.09)   
# cntrySlovakia                                   1.00 ***
#                                                (0.09)   
# --------------------------------------------------------
# R^2                 0.04          0.04          0.10    
# Adj. R^2            0.04          0.04          0.10    
# Num. obs.       14576         14576         14576       
# RMSE                1.59          1.58          1.54    
# ========================================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]

---
# Regression tables

.push-left[

```r
# Regression table of model objects ols1 to ols3; 
# report standard errors, not confidence intervals.
texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.doc") # Word table

texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.html") # Html table

texreg::texreg(list(ols, ols2, ols3), 
       include.ci = FALSE, 
       file =  "MyOLSModels.tex") # Latex table 
```


```r
# Regression table with labelled predicators
htmlreg(list(ols, ols2, ols3), include.ci = FALSE,
        custom.coef.names = c("Intercept", 
                              "Years of Education", 
                              "Age", "Women", 
                              "Denmark", "Norway", "Sweden"),
        caption = "My regression table", 
        caption.above = TRUE,
        single.row = TRUE)
```
]

.push-right[
.font60[

```
# Error in matrixreg(l, single.row = single.row, stars = stars, custom.model.names = custom.model.names, : 'custom.coef.names' must be a character vector of length 13.
```
]]

---
# Further processing of model results

`broom:tidy()` turns a model object into a tibble containing coefficients and inference stats.

```r
tidy(ols3) 
#              term estimate std.error statistic  p.value conf.low conf.high    df outcome
# 1     (Intercept)   7.1324    0.1525    46.755 0.00e+00    6.833    7.4314 14563 imwbcnt
# 2          eduyrs  -0.0975    0.0124    -7.886 3.35e-15   -0.122   -0.0732 14563 imwbcnt
# 3         par_edu  -0.1628    0.0223    -7.314 2.72e-13   -0.206   -0.1192 14563 imwbcnt
# 4      gndrFemale  -0.1475    0.0624    -2.361 1.82e-02   -0.270   -0.0250 14563 imwbcnt
# 5    cntryCzechia   1.0558    0.0792    13.337 2.43e-40    0.901    1.2110 14563 imwbcnt
# 6    cntryEstonia   0.0371    0.0833     0.446 6.56e-01   -0.126    0.2005 14563 imwbcnt
# 7    cntryFinland  -1.0057    0.0745   -13.493 3.06e-41   -1.152   -0.8596 14563 imwbcnt
# 8     cntryFrance  -0.3276    0.0769    -4.260 2.06e-05   -0.478   -0.1769 14563 imwbcnt
# 9    cntryCroatia  -0.4285    0.0945    -4.533 5.87e-06   -0.614   -0.2432 14563 imwbcnt
# 10   cntryHungary   0.5585    0.0713     7.835 5.02e-15    0.419    0.6982 14563 imwbcnt
# 11 cntryLithuania  -0.3525    0.0906    -3.892 9.99e-05   -0.530   -0.1750 14563 imwbcnt
# 12  cntrySlovenia   0.0316    0.0864     0.366 7.15e-01   -0.138    0.2010 14563 imwbcnt
# 13  cntrySlovakia   1.0010    0.0905    11.058 2.59e-28    0.824    1.1784 14563 imwbcnt
```

---
# Regression results are data too!


```r
(plotdata &lt;- ols3 %&gt;% tidy() %&gt;% # Use the ols5 model object, then tidy it, then
   mutate(term = fct_recode(factor(term), # Recode predictor names, then
                            "Years of Education" = "eduyrs",
                            "Parental education" = "par_edu",
                            "Women" = "gndrFemale",
                            "Intercept" = "(Intercept)",
                            "Denmark" = "cntryDenmark",
                            "Norway" = "cntryNorway",
                            "Sweden" = "cntrySweden"),
          predictor = case_when( # Identify explanatry and confounding variables
            term == "Years of Education" ~ "Predictor",
            TRUE ~ "Potential confounder"))) %&gt;% 
  select(term, estimate, conf.low, conf.high, predictor) # keep only some of all the info.
#                  term estimate conf.low conf.high            predictor
# 1           Intercept   7.1324    6.833    7.4314 Potential confounder
# 2  Years of Education  -0.0975   -0.122   -0.0732            Predictor
# 3  Parental education  -0.1628   -0.206   -0.1192 Potential confounder
# 4               Women  -0.1475   -0.270   -0.0250 Potential confounder
# 5        cntryCzechia   1.0558    0.901    1.2110 Potential confounder
# 6        cntryEstonia   0.0371   -0.126    0.2005 Potential confounder
# 7        cntryFinland  -1.0057   -1.152   -0.8596 Potential confounder
# 8         cntryFrance  -0.3276   -0.478   -0.1769 Potential confounder
# 9        cntryCroatia  -0.4285   -0.614   -0.2432 Potential confounder
# 10       cntryHungary   0.5585    0.419    0.6982 Potential confounder
# 11     cntryLithuania  -0.3525   -0.530   -0.1750 Potential confounder
# 12      cntrySlovenia   0.0316   -0.138    0.2010 Potential confounder
# 13      cntrySlovakia   1.0010    0.824    1.1784 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="7-Bivariate-OLS_files/figure-html/unnamed-chunk-23-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata,
       aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high, color = predictor)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange() + # Coefs with 95% CI.
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank())
```
]]

---
class: clear
# Further processing of model results .font60[Several models]

```r
*(plotdata &lt;- bind_rows( # Stack several tibbles on top of each other into one tibble,
* tidy(ols), tidy(ols2), # turn the two models into tibbles,
* .id = "model" # Identify which model the tibbles came from.
)) 
#   model        term estimate std.error statistic  p.value conf.low conf.high    df outcome
# 1     1 (Intercept)   7.2743    0.1473     49.38 0.00e+00    6.986    7.5630 14574 imwbcnt
# 2     1      eduyrs  -0.1528    0.0107    -14.31 4.09e-46   -0.174   -0.1318 14574 imwbcnt
# 3     2 (Intercept)   7.2706    0.1470     49.46 0.00e+00    6.982    7.5588 14573 imwbcnt
# 4     2      eduyrs  -0.1353    0.0120    -11.31 1.48e-29   -0.159   -0.1119 14573 imwbcnt
# 5     2     par_edu  -0.0769    0.0218     -3.52 4.32e-04   -0.120   -0.0341 14573 imwbcnt
```

---
# Further processing of model results


```r
(plotdata &lt;- bind_rows(tidy(ols), tidy(ols2), # Stack the data of two model objects into one tibble.
                       .id = "model") %&gt;% # Identify which model the data came from, then
   # Get rid of the intercept, because it is so large compared to the other coefficients.
   dplyr::filter(term != "(Intercept)") %&gt;%
   mutate(
     model = case_when(model == 1 ~ "Model 1", # Rename model identifyer.
                       model == 2 ~ "Model 2"),
     term = fct_recode(factor(term), # Recode predictor names, then
                       "Years of Education" = "eduyrs",
                       "Parental education" = "par_edu"),
     predictor = case_when( # Identify explanatry and confounding variables
       term == "Years of Education" ~ "Predictor",
       TRUE ~ "Potential confounder")) %&gt;% 
   select(term, estimate, conf.low, conf.high, model, predictor)) # keep only some of all the info.
#                 term estimate conf.low conf.high   model            predictor
# 1 Years of Education  -0.1528   -0.174   -0.1318 Model 1            Predictor
# 2 Years of Education  -0.1353   -0.159   -0.1119 Model 2            Predictor
# 3 Parental education  -0.0769   -0.120   -0.0341 Model 2 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="7-Bivariate-OLS_files/figure-html/unnamed-chunk-27-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata, 
       aes(x = reorder(term, estimate),
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor, shape = model)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
* geom_pointrange(# Coefs &amp; 95% CI.
*   position = position_dodge(width=0.5)) +
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank(),
        legend.box = 'vertical')
```
]]

---
class: inverse
# General lessons

1. Again: everything in R is an object and you can always further process it. For instance, regression results are also just data, which you can visualize, table, join to other data, etc.

---
class: inverse
# Important functions

1. `lm()`: Estimate linear OLS regression. `estimatr::lm_robust()` estimates OLS regression with heteroscedasticity-robust standard errors (or cluster-robust standard errors if you wish).
2. `broom::tidy()`: Return tibble of model results.
3. `textreg::texreg()`, `textreg::htmlreg()`, and `textreg::screenreg()`: Create nicely-formatted (html, Word, ASCII, or Latex) tables of (one or several) regression models.
4. `dplyr::bind_rows()`: Add the rows of a data frames to another data frame that has equal columns/variables.
5. `dplyr::bind_cols()`: Add rows of another data frame to a data frame that has an equal number of rows. In contrast to a join/merge, no identifying key is specified. This can be helpful, but is also a bit dangerous!
6. `scale()` z-standardizes variables. But sometimes it returns a matrix rather than a vector. Therefore it makes sense to always code `scale(x) %&gt;% as.numeric()` to ensure you get an numeric vector out of it.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../template/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
