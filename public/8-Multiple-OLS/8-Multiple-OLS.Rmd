---
title: "Intro to R for Social Data Science"
subtitle: "8 Multiple OLS Regression"
author: "Merlin Schaeffer & Friedolin Merhout <br> Department of Sociology"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    chakra: "../template/remark-latest.min.js"
    css: ["../template/Merlin169.css"]
    lib_dir: libs
    # self_contained: true
    nature:
      highlightLanguage: r
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---
```{r setup, include = FALSE}
library(RefManageR)
library(knitr)
library(equatiomatic)
library(dagitty) # Use the dagitty package
library(ggdag) # Neat visualization of DAGs

options(htmltools.dir.version = FALSE, servr.interval = 0.5, width = 115, digits = 3)
knitr::opts_chunk$set(
  collapse = TRUE, message = FALSE, fig.retina = 3, error = TRUE,
  warning = FALSE, cache = FALSE, fig.align = 'center',
  comment = "#", strip.white = TRUE, tidy = FALSE)

BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           style = "markdown",
           hyperlink = FALSE,
           no.print.fields = c("doi", "url", "ISSN", "urldate", "language", "note", "isbn", "volume"))
myBib <- ReadBib("./../../intRo.bib", check = FALSE)

xaringanExtra::use_xaringan_extra(c("tile_view", "tachyons"))
xaringanExtra::use_panelset()
```
# Let's get ready

```{r}
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(ggplot2) # Allows us to create nice figures.
library(estimatr) # Allows us to estimate (cluster-)robust standard errors.
library(texreg) # Allows us to make nicely-formatted Html & Latex regression tables.
library(broom) # Allows us to turn parts of model objects into tibbles.
library(modelr) # Model predictions (can also do resampling). #<<
```

```{r include = FALSE}
# Import the ESS round 10
ESS <- read_spss("./../../assets/ESS10.sav")
```

---
class: clear

```{r}
ESS <- ESS %>% transmute( # Create new variables and keep only those
    cntry = as_factor(cntry) %>% fct_relevel("France"), # Country of interview #<<
    gndr = as_factor(gndr), # Gender
    agea = zap_labels(agea), # Age #<<
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = zap_labels(pspwght),
    # Parental education (ISCED)
    eiscedf = zap_labels(eiscedf),
    eiscedm = zap_labels(eiscedm),
    par_edu = case_when(
      # If father's edu is missing but not mother's, take mother's
      (eiscedf == 55 | is.na(eiscedf)) & eiscedm != 55 & !is.na(eiscedm) ~ eiscedm,
      # If mother's edu is missing but not father's, take father's
      (eiscedm == 55 | is.na(eiscedm)) & eiscedf != 55 & !is.na(eiscedf)  ~ eiscedf,
      # If both are missing, its missing
      eiscedf == 55 & eiscedm == 55 ~ as.numeric(NA),
      # For all others, take the average of both parents
      TRUE ~ (eiscedm + eiscedf) / 2),
    eduyrs = case_when( # Education
      eduyrs > 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs < 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %>% # Make it numeric, then
  dplyr::filter(
    facntr == "Yes" & mocntr == "Yes")
```

---
# Design matrix

```{r}
# Design matrix (i.e., tibble of the vars going into out model)
ESS <- ESS %>%
  select(cntry, imwbcnt, eduyrs, par_edu, gndr, agea, pspwght) %>%
  mutate(cntry = fct_drop(cntry)) %>% # Drop unused cntry factor levels, then
  drop_na() # Casewise deletion.
```

---
# OLS I $\rightarrow$ OLS II

.left-column[
**OLS I**: OLS allows us to express a continuous outcome as *linear* function of continuous or categorical predictors. We can adjust/control for multiple confounding variables. The estimated linear association is *not conditional* on the level of the control/confounding variables.

**OLS II**: Conditional and non-linear relationships!
]

.right-column[
.font80[
```{r}
# Run a series of three post-stratification weighted linear OLS models.
ols1 <- lm_robust(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
ols2 <- lm_robust(formula = imwbcnt ~ eduyrs + par_edu, data = ESS, weights = pspwght)
ols3 <- lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, data = ESS, weights = pspwght)
# Report results in a regression table.
screenreg(list(ols1, ols2, ols3), include.ci = FALSE) 
```
]]

---
# Conditional relations .font60[Aka interaction effects]

.panelset[
.panel[.panel-name[Plot]
```{r out.width='60%', fig.height = 4, fig.width = 7, echo = FALSE}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = eduyrs, 
                     color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), 
              method = "lm", se = FALSE) +
  labs(x = "Years of education",
       y = "Xenophobia",
       size = "Survey \n weight", 
       color = "Country") +
  theme_minimal()
```
]

.panel[.panel-name[Code]
```{r fig.show = "hide"}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, x = eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  labs(x = "Years of education", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]]

---
class: clear
# Interaction effects .font60[Specify them as what they are: *multiplicative* terms]

.right-column[
.font70[
```{r}
ols4 <- lm_robust(formula = imwbcnt ~ par_edu + gndr +
                    eduyrs*cntry, #<< 
                  data = ESS, weights = pspwght)
screenreg(ols4, include.ci = FALSE) 
```
]]

--

.left-column[.content-box-green[

1. How do we interpret the estimate `eduyrs:cntrySweden` correctly?

2. Why is $\beta_{\text{eduyrs}}$ more negative than in model 3?

3. Why do Bulgarians suddenly seem to have *lower* levels of xenophobia, whereas before they had higher ones?
]]

---
# Interaction effects .font60[Beware of the scaling of the Xs!]

.panelset[
.panel[.panel-name[Plot]
```{r out.width='60%', fig.height = 4, fig.width = 7, echo = FALSE}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = eduyrs, 
                     color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), 
              method = "lm", se = FALSE,
              # draw line beyond observed cases. #<<
              fullrange = TRUE) + #<<
  xlim(c(0, 21)) + # Extend range of x-axis. #<<
  labs(x = "Years of education",
       y = "Xenophobia",
       size = "Survey \n weight", 
       color = "Country") +
  theme_minimal()
```
]
.panel[.panel-name[Code]
```{r fig.show = "hide"}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, x = eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE,
              # draw line beyond observed cases. #<<
              fullrange = TRUE) + #<<
  xlim(c(0, 21)) + # Extend range of x-axis. #<<
  labs(x = "Years of education", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]]

---
# Solution: Rescale .font60[(e.g., z-standardization)]

.panelset[
.panel[.panel-name[Code]
```{r}
ESS <- ESS %>% mutate( 
  # create z-standardized version of continuous predictor.
  z_eduyrs = scale(eduyrs) %>% as.numeric()
)
```

```{r z-standardize, fig.show = "hide"}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, x = z_eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_vline(xintercept = 0, color = "#901A1E") + 
  geom_smooth(aes(weight = pspwght), 
              method = "lm", se = FALSE) +
  labs(x = "Years of education \n (z-standardized)", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]
.panel[.panel-name[Plot]
```{r ref.label = "z-standardize", out.width='60%', fig.height = 4, fig.width = 6, echo = FALSE}
```
]]

---
# Interaction effects .font60[Improved interpretability]

.push-right[
.font70[
```{r}
ols4 <- lm_robust(formula = imwbcnt ~ par_edu + gndr +
                    I(scale(eduyrs))*cntry, ##<
                  data = ESS, weights = pspwght)
screenreg(ols4, include.ci = FALSE) 
```
]]

.push-left[
```{r ref.label = "z-standardize", out.width='100%', fig.height = 4, fig.width = 6, echo = FALSE}
```
]

---
# Non-linearities? .font60[Transform using `I()` and `poly()`]

.push-left[
`I()` allows you to perform any transformation without a need to change the underlying tibble/design matrix. Really helpful for predictions!

`poly()` introduces *orthogonal* polynomials (e.g., squared terms). If you don't want them to be orthogonal, you can use the option `raw = TRUE`.

.alert[Beware of over-fitting!]
```{r}
ols6 <- lm_robust(
  formula = imwbcnt ~ gndr + cntry + eduyrs +
    poly(agea, 2, raw = TRUE) + #<<
    I(log(par_edu)), #<<
  data = ESS, weights = pspwght)
```
]

.push-right[
.font80[
```{r}
screenreg(ols6, include.ci = FALSE, digits = 4) 
```
]]

---
# Model visualization .font60[Especially for lay audiences!]

.push-left[
**Step 1**: Generate a synthetic tibble with theoretically-informative values containing all variables of the model you want to visualize. `modelr::data_grid()` generates a tibble with all combinations of the variables you feed to it.
.font80[
```{r}
(synth_data <- ESS %>%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
   data_grid(eduyrs, par_edu, gndr, agea, cntry)) #<<
```
]]

--

.push-right[
.font80[
**Step 2**: Alter your new tibble, so that only one predictor of interest varies. Set all others to an informative constant value.
```{r}
(synth_data <- synth_data %>%
   mutate( # Set variables constant that you don't want to vidualize,
     eduyrs = mean(ESS$eduyrs, na.rm = TRUE),
     par_edu = mean(ESS$par_edu, na.rm = TRUE),
     gndr = "Female",
     cntry = "Denmark") %>% # then
   unique()) # Drop duplicates.
```
]]

---
# Model predictions

.push-left[
**Step 3**: Add predictions, that is, apply the estimated model to the synthetic data. Given the synthetic data, what $\hat{y}$ does your model predict?
```{r synth_data, eval = FALSE}
# Generate predictions and 95% CI, then
(synth_data <- predict(ols6, newdata = synth_data, #<<
                       interval = "confidence", #<<
                       level = 0.95)$fit %>% #<<
   as_tibble() %>% # Turn into a tibble, then
   # Add to the synthetic data.
   bind_cols(synth_data, .)) 
```
]

.push-right[
.font80[
```{r ref.label = "synth_data", echo = FALSE}
```
]]

---
# Visualize model predictions

.push-left[
.panelset[
.panel[.panel-name[Plot]
```{r echo = FALSE, out.width='90%', fig.height = 3.5, fig.width = 6}
ggplot(data = synth_data, 
       aes(y = fit, x = agea)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              alpha = 0.5) +
  geom_line() +
  labs(x = "Age",
  y = "Xenophobia",
  caption = "(Covariates set to: Women in Denmark \n at mean levels of (parental) education)") + 
  theme_minimal()
```
]
.panel[.panel-name[Code]
```{r fig.show = 'hide'}
ggplot(data = synth_data, 
       aes(y = fit, x = agea)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              alpha = 0.5) +
  geom_line() +
  labs(x = "Age",
  y = "Xenophobia",
  caption = "(Covariates set to: Women in Denmark \n at mean levels of (parental) education)") + 
  theme_minimal()
```
]]]
.push-right[
.font80[
```{r}
screenreg(ols6, include.ci = FALSE, digits = 4) 
```
]]

---
# One more time ...

.push-left[
.font90[
```{r}
# Generate synthetic data to visualize parental education
synth_data <- ESS %>%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
   data_grid(eduyrs, par_edu, gndr, agea, cntry) %>%
   mutate( # Set confounders constant at their mean,
     eduyrs = mean(ESS$eduyrs, na.rm = TRUE),
     agea = mean(ESS$agea, na.rm = TRUE),
     gndr = "Female",
     cntry = "Denmark") %>% # then
   unique() # Drop duplicates.

# Generate predictions and 95% confidence intervals, then
synth_data <- predict(ols6, newdata = synth_data,
                      interval = "confidence",
                      level=0.95)$fit %>%
  as_tibble() %>% # Turn into a tibble, then
  bind_cols(synth_data, .) # Add to the synthetic data.
```
]]

.push-right[
.panelset[
.panel[.panel-name[Plot]
```{r echo = FALSE, out.width='90%', fig.height = 3.5, fig.width = 6}
ggplot(data = synth_data, 
       aes(y = fit, x = par_edu)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              alpha = 0.5) +
  geom_line() +
  labs(x = "Parental education",
  y = "Xenophobia",
  caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]
.panel[.panel-name[Code]
.font90[
```{r fig.show = 'hide'}
ggplot(data = synth_data, aes(y = fit, x = par_edu)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.5) +
  geom_line() +
  labs(x = "Parental education", y = "Xenophobia", 
       caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]]]]
---
# Visualizing interactions

.push-left[
.font90[
```{r}
# Estimate linear OLS with interaction effect;
# no z-standardization if you visualize.
ols7 <- lm_robust(formula = imwbcnt ~ log(par_edu) + gndr + poly(agea, 2) +
                    eduyrs*cntry, #<<
                  data = ESS, weights = pspwght)

# Generate synthetic data to 
# visualize parental education
synth_data <- ESS %>%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
   data_grid(par_edu, gndr, cntry, eduyrs, agea) %>%
   mutate( # Set confounders constant at their mean,
     # but let both cntry and eduyrs vary. #<<
     par_edu = mean(ESS$par_edu, na.rm = TRUE),
     agea = mean(ESS$agea, na.rm = TRUE),
     gndr = "Female") %>% # then
   unique() # Drop duplicates.

# Generate predictions and 95% confidence intervals, then
synth_data <- predict(ols7, newdata = synth_data,
                      interval = "confidence",
                      level=0.95)$fit %>%
  as_tibble() %>% # Turn into a tibble, then
  bind_cols(synth_data, .) # Add to the synthetic data.
```
]]

.push-right[
.font80[
```{r echo = FALSE}
print(synth_data, n = 28)
```
]]

---
# Visualizing interactions

.panelset[
.panel[.panel-name[Plot]
```{r echo = FALSE, out.width='65%', fig.height = 3.5, fig.width = 6}
ggplot(data = synth_data, 
       aes(y = fit, x = eduyrs)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, 
                  fill = cntry), #<<
              alpha = 0.5) +
  geom_line(aes(color = cntry)) + #<<
  labs(x = "Years of education",
  y = "Xenophobia",
  caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]
.panel[.panel-name[Code]
```{r fig.show = 'hide'}
ggplot(data = synth_data, aes(y = fit, x = eduyrs)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, 
                  fill = cntry), #<<
              alpha = 0.5) +
  geom_line(aes(color = cntry)) + #<<
  labs(x = "Years of education", y = "Xenophobia", 
       caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]]

---
# Marginal effects 

<iframe src="https://www.rdocumentation.org/packages/margins/versions/0.3.26" height="100%" width="100%" frameBorder="0" style="min-height: 560px;"></iframe>

---
background-image: url(https://mareds.github.io/r_course/img_site/Tidyverse_packages.png)
background-size: contain
background-position: center
class: clear center

--

<br>
.alert[.font200[Thank you for a fun course!]]

---
class: inverse
# General lessons

1. Linear OLS models are very versatile and can approximate very complex relationships. Theoretically, interaction effects, transformed variables and polynomials allow us to *linearize* any relationship/functional form.
2. You can formulate conditional and non-linear relationships directly in your model's formula.
3. It is important to visualize such complex relationships, because regression coefficients alone are hard to interpret. You can do that by making predictions from your model for synthetic data.

---
class: inverse
# Important functions

1. `I()` allows you to transform predictors in the formula argument of your regression model.
2. `poly()` allows you to introduce orthogonal polynomials to your model.
3. `predict()` generates predictions from your model for a tibble that you feed to it.
4. `modelr::data_grid()` generates synthetic data sets that contain all combinations of variables you feed to it.
