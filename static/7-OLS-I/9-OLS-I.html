<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Intro to R for Social Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Merlin Schaeffer  Department of Sociology" />
    <meta name="date" content="2021-03-07" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="libs/Merlin169.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to R for Social Data Science
## Linear OLS Regression I
### Merlin Schaeffer<br> Department of Sociology
### 2021-03-07

---


# Let's get ready


```r
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(essurvey) # Add ESS API package.
library(ggplot2) # Allows us to create nice figures.
library(factoextra) # Allows us to visualize PCAs.
library(psych) # Supports PCA &amp; factor analysis with several useful functions.
*library(estimatr) # Allows us to estimate (cluster-)robust standard errors.
*library(texreg) # Allows us to make nicely-formatted Html &amp; Latex regression tables.
*library(broom) # Allows us to turn model objects into tibbles.
```



---
class: clear


```r
ESS &lt;- ESS %&gt;% transmute( # Create new variables and keep only those
    cntry = as_factor(cntry), # Country of interview
    gndr = as_factor(gndr),
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imbgeco = max(imbgeco, na.rm = TRUE) - zap_labels(imbgeco),
    imueclt = max(imueclt, na.rm = TRUE) - zap_labels(imueclt),
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = zap_label(pspwght),
*   # Parental education (ISCED)
*   eiscedf = zap_labels(eiscedf),
*   eiscedm = zap_labels(eiscedm),
*   par_edu = case_when(
*     # If father's edu is missing but not mother's, take mother's
*     (eiscedf == 55 | is.na(eiscedf)) &amp; eiscedm != 55 &amp; !is.na(eiscedm) ~ eiscedm,
*     # If mother's edu is missing but not father's, take father's
*     (eiscedm == 55 | is.na(eiscedm)) &amp; eiscedf != 55 &amp; !is.na(eiscedf)  ~ eiscedf,
*     # If both are missing, its missing
*     eiscedf == 55 &amp; eiscedm == 55 ~ as.numeric(NA),
*     # For all others, take the average of both parents
*     TRUE ~ (eiscedm + eiscedf) / 2),
    eduyrs = case_when( # Education
      eduyrs &gt; 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs &lt; 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %&gt;% # Make it numeric, then
  dplyr::filter(
    # Keep only respondents with native-born parents,
    facntr == "Yes" &amp; mocntr == "Yes" &amp;
*     (cntry == "Denmark" | cntry == "Sweden" | cntry == "Norway" | cntry == "Germany"))
```

---
# PCA &amp; design matrix


```r
# Perform pca of three items and write the results into object xeno.
(xeno &lt;- ESS %&gt;% 
   prcomp(formula = ~ imbgeco + imueclt + imwbcnt, 
          data = ., scale = TRUE, center = TRUE,
          na.action = na.exclude))
# Standard deviations (1, .., p=3):
# [1] 1.514 0.659 0.525
# 
# Rotation (n x k) = (3 x 3):
#           PC1    PC2     PC3
# imbgeco 0.552 -0.833  0.0422
# imueclt 0.588  0.425  0.6886
# imwbcnt 0.591  0.355 -0.7239

# Make a new "xeno" variable and fill it with z-standardized PC1.
ESS$xeno &lt;- xeno$x[ , "PC1"] %&gt;% # Take PC1 scores, then
  scale() %&gt;% # z-standardize (but sometimes returns a "matrix"), then 
  as.numeric() # turn into numeric vector.

# Design matrix (i.e., tibble of the vars going into out model)
ESS &lt;- ESS %&gt;%
  select(cntry, xeno, eduyrs, par_edu, gndr, pspwght) %&gt;%
  drop_na() # Casewise deletion.
```

---
# Modeling Xenophobia

.push-left[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
.panel[.panel-name[Code]

```r
# Plot the distribution of xeno
ggplot(data = ESS, aes(x = xeno)) +
  geom_freqpoly() +
  theme_minimal()
```
]
]]]

--

.push-right[
#### Modeling
Coming up with a model of xenophobia means: Can xenophobia (i.e., `\(y\)`) be expressed as a function (i.e., `\(f()\)`) of another variable (i.e., `\(x\)`)?

`$$\hat{y} = f(x)$$`
The two critical questions are thus:

1. What is `\(f()\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]
]

---
# A linear Model of `\(y\)` .font60[Simple, but extremely versatile!]

.right-column[
`$$\hat{y} = \alpha + \beta x$$`

&lt;img src="./img/LinearModel.png" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.left-column[
Now, the two critical questions have turned to:
1. What are `\(\alpha\)` and `\(\beta\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]

]

---
# A linear Model of `\(y\)`

.left-column[
Let's try `\(x = \text{years of education}\)`, since schools in most countries teach humanism and tolerance: Is xenophobia a linear (i.e., constant slope) function of one's years of education?
]

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-8-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, mapping = aes(y = xeno, x = eduyrs)) +
  geom_jitter(aes(size = pspwght), alpha = 1/5) + 
  geom_smooth(aes(weight = pspwght), se = FALSE) +
  geom_smooth(aes(weight = pspwght), method = "lm", 
              se = FALSE, color = "red") +
  labs(y = "Xenophobia", x =  "Years of education", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Ordinary Least Squares

.left-column[
We find `\(\alpha\)` and `\(\beta\)` by choosing the one line that minimizes the *residual sum of squares*:

`$$\begin{align*}
      \min \text{RSS} &amp;= \min \sum_{i=1}^{n} e_{i}^{2} \\
      &amp;= \min \sum_{i=1}^{n} y_{i} - \hat{y_{i}} \\
      &amp;= \min \sum_{i=1}^{n} (y_{i} - (\alpha + \beta x_{i})^{2}
    \end{align*}$$`
    
.backgrnote[
Note, how the residuals are defined only in terms of `\(y\)` and thereby differ fundamentally from those that PCA minimizes!
]
]

.right-column[
&lt;img src="9-OLS-I_files/figure-html/OLS-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---
# The `lm()` function 

.right-column[

```r
# Estimate a linear OLS model and assign the results to object ols.
ols &lt;- lm(formula = xeno ~ eduyrs, data = ESS)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = xeno ~ eduyrs, data = ESS)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -2.421 -0.671 -0.073  0.584  3.235 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.00191    0.05812    17.2   &lt;2e-16 ***
# eduyrs      -0.07090    0.00397   -17.9   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 0.969 on 5229 degrees of freedom
# Multiple R-squared:  0.0575,	Adjusted R-squared:  0.0573 
# F-statistic:  319 on 1 and 5229 DF,  p-value: &lt;2e-16
```
]

.left-column[

$$$$
\operatorname{\widehat{xeno}} = 1 - 0.07(\operatorname{eduyrs})
`$$,$$` is the line that `\(\min \sum_{i=1}^{n} e_{i}^{2}\)`.

]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use (post-stratification) weights. 

.alert[But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values, and confidence intervals) will be wrong, because weights introduce heteroscedasticity.]
]

.right-column[

```r
# Estimate a weighted linear OLS model.
ols &lt;- lm(formula = xeno ~ eduyrs, data = ESS, 
*         weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = xeno ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted Residuals:
#    Min     1Q Median     3Q    Max 
# -3.616 -0.626 -0.085  0.513  4.981 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.03691    0.05869    17.7   &lt;2e-16 ***
# eduyrs      -0.07181    0.00414   -17.4   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 0.97 on 5229 degrees of freedom
# Multiple R-squared:  0.0544,	Adjusted R-squared:  0.0543 
# F-statistic:  301 on 1 and 5229 DF,  p-value: &lt;2e-16
```
]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use post-stratification weights.

But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values) will be wrong, because weights introduce heteroscedasticity.

.alert[We thus need to estimate heteroscedasticity-robust standard errors.] One comfortable possibility to do that is `estimatr::lm_robust()`.
]

.right-column[

```r
# Estimate a weighted linear OLS model.
*ols &lt;- lm_robust(formula = xeno ~ eduyrs, data = ESS,
                 weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = xeno ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)   1.0369    0.07097    14.6 2.02e-47   0.8978   1.1760 5229
# eduyrs       -0.0718    0.00476   -15.1 2.09e-50  -0.0811  -0.0625 5229
# 
# Multiple R-squared:  0.0544 ,	Adjusted R-squared:  0.0543 
# F-statistic:  228 on 1 and 5229 DF,  p-value: &lt;2e-16
```
]

---
# Multiple linear OLS

.left-column[
&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;

.alert[But is parental education the only confounder?] 

.backgrnote[
A confounder is a common cause of both one's predictor `\(x_{i1}\)` (e.g., years of education) and of the outcome `\(y_i\)` (e.g., xenophobia). Counfounder bias is: `\(\mathbb{E}(\tilde{\beta_{x_{1}}}) = \beta_{x_{1}} + \color{orange}{\beta_{x_{2}} \frac{\text{Cov}(x_{i1},x_{i2})}{\text{Var}(x_{i1})}}.\)`
]
]

.right-column[
In the social sciences we use OLS to test theories about causal effects. It is thus important to rule out alternative explanations.

```r
# Estimate a weighted linear OLS model.
ols2 &lt;- lm_robust(
* formula = xeno ~ eduyrs + par_edu,
  data = ESS, weights = pspwght)
summary(ols2) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = xeno ~ eduyrs + par_edu, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)   1.1887    0.07076   16.80 1.01e-61   1.0500   1.3275 5228
# eduyrs       -0.0592    0.00482  -12.28 3.34e-34  -0.0687  -0.0498 5228
# par_edu      -0.0972    0.01006   -9.66 6.51e-22  -0.1169  -0.0775 5228
# 
# Multiple R-squared:  0.0771 ,	Adjusted R-squared:  0.0768 
# F-statistic:  168 on 2 and 5228 DF,  p-value: &lt;2e-16
```
]

---
# Confounder bias .font60[A simulation]


```r
set.seed(838377)
# The Data Generating Process (DGP)
simu &lt;- tibble(Confounder = rnorm(n = 2000, mean = 0, sd = 1), # The confounder is exogeneous.
               x = 0.5*Confounder + rnorm(n = 2000, mean = 0, sd = 1), # x is a "child" of the confounder.
               y = 0.5*x + 0.5*Confounder + rnorm(n = 2000, mean = 0, sd = 1)) # y is a child of x and the confounder.
```

--

.left-column[
.center[.content-box-blue[.font70[
Based on `\(\mathbb{E}(\tilde{\beta_{x_{1}}}) = \beta_{x_{1}} + \color{orange}{\beta_{x_{2}} \frac{\text{Cov}(x_{i1},x_{i2})}{\text{Var}(x_{i1})}}\)`, what bias should we expect?
]]]

&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-16-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

.right-column[
.panelset[
.panel[.panel-name[Correct model]
.font80[

```r
lm_robust(y ~ x + Confounder, data = simu) %&gt;%
  summary(single.row = TRUE)
# 
# Call:
# lm_robust(formula = y ~ x + Confounder, data = simu)
# 
# Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)  0.00296     0.0221   0.134  8.94e-01  -0.0404   0.0464 1997
# x            0.51470     0.0226  22.813 1.45e-102   0.4705   0.5590 1997
# Confounder   0.48869     0.0240  20.377  5.21e-84   0.4417   0.5357 1997
# 
# Multiple R-squared:  0.446 ,	Adjusted R-squared:  0.445 
# F-statistic:  816 on 2 and 1997 DF,  p-value: &lt;2e-16
```
]]
.panel[.panel-name[Biased model]
.font80[

```r
lm_robust(y ~ x, data = simu) %&gt;%
  summary(single.row = TRUE)
# 
# Call:
# lm_robust(formula = y ~ x, data = simu)
# 
# Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)   -0.020     0.0243  -0.824  4.10e-01  -0.0676   0.0276 1998
# x              0.704     0.0223  31.506 3.17e-177   0.6603   0.7479 1998
# 
# Multiple R-squared:  0.334 ,	Adjusted R-squared:  0.334 
# F-statistic:  993 on 1 and 1998 DF,  p-value: &lt;2e-16
```
]]]]

---
# Categorical predictors

.left-column[
.center[.content-box-blue[
How can we make sense of a categorical predictor in a linear model?]]
]

--

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="9-OLS-I_files/figure-html/dummies-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, 
       mapping = aes(y = xeno, 
                     x = case_when(
                       cntry == "Denmark" ~ 0,
                       cntry == "Sweden" ~ 1,
                       TRUE ~ as.numeric(NA)
                     ))) +
  geom_jitter(aes(size = pspwght), alpha = 1/15, width = 0.05, height = 0) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(0, 1), 
                     labels = c("0 = Denmark", "1 = Sweden")) +
  labs(x = "Country", y = "Xenophobia", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Categorical predictors

.left-column[
Factors are automatically treated as dummies, with the first level taken as reference. Thus if you want to change the reference, you need to recode the levels of the factor using the `forcats` package.
]

.right-column[
.font90[

```r
ols3 &lt;- lm_robust(formula = xeno ~ eduyrs + par_edu + gndr + cntry, 
                  data = ESS, weights = pspwght)
summary(ols3) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = xeno ~ eduyrs + par_edu + gndr + cntry, data = ESS, 
#     weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)  CI Lower CI Upper   DF
# (Intercept)    1.3255    0.07536   17.59 2.57e-67  1.18e+00  1.47323 5224
# eduyrs        -0.0606    0.00472  -12.83 3.90e-37 -6.98e-02 -0.05133 5224
# par_edu       -0.0914    0.00982   -9.31 1.90e-20 -1.11e-01 -0.07214 5224
# gndrFemale    -0.1383    0.03038   -4.55 5.47e-06 -1.98e-01 -0.07870 5224
# cntryDenmark   0.0829    0.04232    1.96 5.01e-02 -3.58e-05  0.16591 5224
# cntryNorway   -0.0826    0.03932   -2.10 3.57e-02 -1.60e-01 -0.00553 5224
# cntrySweden   -0.3599    0.04245   -8.48 2.91e-17 -4.43e-01 -0.27672 5224
# 
# Multiple R-squared:  0.106 ,	Adjusted R-squared:  0.105 
# F-statistic: 73.9 on 6 and 5224 DF,  p-value: &lt;2e-16
```
]]

---
# Regression tables

.font90[

```r
# Regression table of model objects ols1 to ols3; report standard errors, not confidence intervals.
texreg::screenreg(list(ols, ols2, ols3), include.ci = FALSE) 
# 
# ===================================================
#               Model 1      Model 2      Model 3    
# ---------------------------------------------------
# (Intercept)      1.04 ***     1.19 ***     1.33 ***
#                 (0.07)       (0.07)       (0.08)   
# eduyrs          -0.07 ***    -0.06 ***    -0.06 ***
#                 (0.00)       (0.00)       (0.00)   
# par_edu                      -0.10 ***    -0.09 ***
#                              (0.01)       (0.01)   
# gndrFemale                                -0.14 ***
#                                           (0.03)   
# cntryDenmark                               0.08    
#                                           (0.04)   
# cntryNorway                               -0.08 *  
#                                           (0.04)   
# cntrySweden                               -0.36 ***
#                                           (0.04)   
# ---------------------------------------------------
# R^2              0.05         0.08         0.11    
# Adj. R^2         0.05         0.08         0.10    
# Num. obs.     5231         5231         5231       
# RMSE             0.97         0.96         0.94    
# ===================================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]

---
# Regression tables

.push-left[

```r
# Regression table of model objects ols1 to ols3; 
# report standard errors, not confidence intervals.
texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.doc") # Word table

texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.html") # Html table

texreg::texreg(list(ols, ols2, ols3), 
       include.ci = FALSE, 
       file =  "MyOLSModels.tex") # Latex table 
```


```r
# Regression table with labelled predicators
htmlreg(list(ols, ols2, ols3), include.ci = FALSE,
        custom.coef.names = c("Intercept", 
                              "Years of Education", 
                              "Age", "Women", 
                              "Denmark", "Norway", "Sweden"),
        caption = "My regression table", 
        caption.above = TRUE,
        single.row = TRUE)
```
]

.push-right[
.font60[
&lt;table class="texreg" style="margin: 10px auto;border-collapse: collapse;border-spacing: 0px;color: #000000;border-top: 2px solid #000000;"&gt;
&lt;caption&gt;My regression table&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 1&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 2&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr style="border-top: 1px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Intercept&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;1.04 (0.07)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;1.19 (0.07)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;1.33 (0.08)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Years of Education&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.07 (0.00)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.06 (0.00)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.06 (0.00)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Age&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.10 (0.01)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.09 (0.01)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Women&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.14 (0.03)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Denmark&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.08 (0.04)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Norway&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.08 (0.04)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Sweden&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.36 (0.04)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="border-top: 1px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.05&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.08&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Adj. R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.05&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.08&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Num. obs.&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5231&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5231&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5231&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="border-bottom: 2px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;RMSE&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.97&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.96&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.94&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="font-size: 0.8em;" colspan="4"&gt;&lt;sup&gt;***&lt;/sup&gt;p &amp;lt; 0.001; &lt;sup&gt;**&lt;/sup&gt;p &amp;lt; 0.01; &lt;sup&gt;*&lt;/sup&gt;p &amp;lt; 0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
]]

---
# Further processing of model results

`broom:tidy()` turns a model object into a tibble containing coefficients and inference stats.

```r
tidy(ols3) 
#           term estimate std.error statistic  p.value  conf.low conf.high   df outcome
# 1  (Intercept)   1.3255   0.07536     17.59 2.57e-67  1.18e+00   1.47323 5224    xeno
# 2       eduyrs  -0.0606   0.00472    -12.83 3.90e-37 -6.98e-02  -0.05133 5224    xeno
# 3      par_edu  -0.0914   0.00982     -9.31 1.90e-20 -1.11e-01  -0.07214 5224    xeno
# 4   gndrFemale  -0.1383   0.03038     -4.55 5.47e-06 -1.98e-01  -0.07870 5224    xeno
# 5 cntryDenmark   0.0829   0.04232      1.96 5.01e-02 -3.58e-05   0.16591 5224    xeno
# 6  cntryNorway  -0.0826   0.03932     -2.10 3.57e-02 -1.60e-01  -0.00553 5224    xeno
# 7  cntrySweden  -0.3599   0.04245     -8.48 2.91e-17 -4.43e-01  -0.27672 5224    xeno
```

---
# Regression results are data too!


```r
(plotdata &lt;- ols3 %&gt;% tidy() %&gt;% # Use the ols5 model object, then tidy it, then
   mutate(term = fct_recode(factor(term), # Recode predictor names, then
                            "Years of Education" = "eduyrs",
                            "Parental education" = "par_edu",
                            "Women" = "gndrFemale",
                            "Intercept" = "(Intercept)",
                            "Denmark" = "cntryDenmark",
                            "Norway" = "cntryNorway",
                            "Sweden" = "cntrySweden"),
          predictor = case_when( # Identify explanatry and confounding variables
            term == "Years of Education" | term == "Years of education^2" ~ "Predictor",
            TRUE ~ "Potential confounder"))) %&gt;% 
  select(term, estimate, conf.low, conf.high, predictor) # keep only some of all the info.
#                 term estimate  conf.low conf.high            predictor
# 1          Intercept   1.3255  1.18e+00   1.47323 Potential confounder
# 2 Years of Education  -0.0606 -6.98e-02  -0.05133            Predictor
# 3 Parental education  -0.0914 -1.11e-01  -0.07214 Potential confounder
# 4              Women  -0.1383 -1.98e-01  -0.07870 Potential confounder
# 5            Denmark   0.0829 -3.58e-05   0.16591 Potential confounder
# 6             Norway  -0.0826 -1.60e-01  -0.00553 Potential confounder
# 7             Sweden  -0.3599 -4.43e-01  -0.27672 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-27-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata,
       aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high, color = predictor)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange() + # Coefs with 95% CI.
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank())
```
]]

---
class: clear
# Further processing of model results .font60[Several models]

```r
*(plotdata &lt;- bind_rows( # Stack several tibbles on top of each other into one tibble,
* tidy(ols), tidy(ols2), # turn the two models into tibbles,
* .id = "model" # Identify which model the tibbles came from.
)) 
#   model        term estimate std.error statistic  p.value conf.low conf.high   df outcome
# 1     1 (Intercept)   1.0369   0.07097     14.61 2.02e-47   0.8978    1.1760 5229    xeno
# 2     1      eduyrs  -0.0718   0.00476    -15.09 2.09e-50  -0.0811   -0.0625 5229    xeno
# 3     2 (Intercept)   1.1887   0.07076     16.80 1.01e-61   1.0500    1.3275 5228    xeno
# 4     2      eduyrs  -0.0592   0.00482    -12.28 3.34e-34  -0.0687   -0.0498 5228    xeno
# 5     2     par_edu  -0.0972   0.01006     -9.66 6.51e-22  -0.1169   -0.0775 5228    xeno
```

---
# Further processing of model results


```r
(plotdata &lt;- bind_rows(tidy(ols), tidy(ols2), # Stack the data of two model objects into one tibble.
                       .id = "model") %&gt;% # Identify which model the data came from, then
   # Get rid of the intercept, because it is so large compared to the other coefficients.
   dplyr::filter(term != "(Intercept)") %&gt;%
   mutate(
     model = case_when(model == 1 ~ "Model 1", # Rename model identifyer.
                       model == 2 ~ "Model 2"),
     term = fct_recode(factor(term), # Recode predictor names, then
                       "Years of Education" = "eduyrs",
                       "Parental education" = "par_edu"),
     predictor = case_when( # Identify explanatry and confounding variables
       term == "Years of Education" ~ "Predictor",
       TRUE ~ "Potential confounder")) %&gt;% 
   select(term, estimate, conf.low, conf.high, model, predictor)) # keep only some of all the info.
#                 term estimate conf.low conf.high   model            predictor
# 1 Years of Education  -0.0718  -0.0811   -0.0625 Model 1            Predictor
# 2 Years of Education  -0.0592  -0.0687   -0.0498 Model 2            Predictor
# 3 Parental education  -0.0972  -0.1169   -0.0775 Model 2 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="9-OLS-I_files/figure-html/unnamed-chunk-31-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata, 
       aes(x = reorder(term, estimate),
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor, shape = model)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
* geom_pointrange(# Coefs &amp; 95% CI.
*   position = position_dodge(width=0.5)) +
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank(),
        legend.box = 'vertical')
```
]]

---
class: inverse
# Today's general lessons

1. Again: everything in R is an object and you can always further process it. For instance, regression results are also just data, which you can visualize, table, join to other data, etc.

---
class: inverse
# Today's (important) functions

1. `lm()`: Estimate linear OLS regression. `estimatr::lm_robust()` estimates OLS regression with heteroscedasticity-robust standard errors (or cluster-robust standard errors if you wish).
2. `broom::tidy()`: Return tibble of model results.
3. `textreg::texreg()`, `textreg::htmlreg()`, and `textreg::screenreg()`: Create nicely-formatted (html, Word, ASCII, or Latex) tables of (one or several) regression models.
4. `dplyr::bind_rows()`: Add the rows of a data frames to another data frame that has equal columns/variables.
5. `dplyr::bind_cols()`: Add rows of another data frame to a data frame that has an equal number of rows. In contrast to a join/merge, no identifying key is specified. This can be helpful, but is also a bit dangerous!
6. `scale()` z-standardizes variables. But sometimes it returns a matrix rather than a vector. Therefore it makes sense to always code `scale(x) %&gt;% as.numeric()` to ensure you get an numeric vector out of it.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
