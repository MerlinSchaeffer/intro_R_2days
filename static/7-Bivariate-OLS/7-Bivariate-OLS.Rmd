---
title: "Intro to R for Social Data Science"
subtitle: "7 Bivariate OLS Regression"
author: "Merlin Schaeffer & Friedolin Merhout <br> Department of Sociology"
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    chakra: "../template/remark-latest.min.js"
    css: ["../template/Merlin169.css"]
    lib_dir: libs
    # self_contained: true
    nature:
      highlightLanguage: r
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---
```{r setup, include = FALSE}
library(RefManageR)
library(knitr)
library(equatiomatic)
library(dagitty) # Use the dagitty package
library(ggdag) # Neat visualization of DAGs

options(htmltools.dir.version = FALSE, servr.interval = 0.5, width = 115, digits = 3)
knitr::opts_chunk$set(
  collapse = TRUE, message = FALSE, fig.retina = 3, error = TRUE,
  warning = FALSE, cache = FALSE, fig.align = 'center',
  comment = "#", strip.white = TRUE, tidy = FALSE)

BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           style = "markdown",
           hyperlink = FALSE,
           no.print.fields = c("doi", "url", "ISSN", "urldate", "language", "note", "isbn", "volume"))
myBib <- ReadBib("./../../intRo.bib", check = FALSE)

xaringanExtra::use_xaringan_extra(c("tile_view", "tachyons"))
xaringanExtra::use_panelset()
```
# Let's get ready

```{r}
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(ggplot2) # Allows us to create nice figures.
library(estimatr) # Allows us to estimate (cluster-)robust standard errors. #<<
library(texreg) # Allows us to make nicely-formatted Html & Latex regression tables. #<<
library(broom) # Allows us to turn model objects into tibbles. #<<
```

```{r include = FALSE}
# Import the ESS round 10
ESS <- read_spss("./../../assets/ESS10.sav")
```

---
class: clear

```{r}
ESS <- ESS %>% transmute( # Create new variables and keep only those
    cntry = as_factor(cntry) %>% fct_drop(), # Country of interview
    gndr = as_factor(gndr),
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = dweight*pweight,
    # Parental education (ISCED) #<<
    eiscedf = zap_labels(eiscedf), #<<
    eiscedm = zap_labels(eiscedm), #<<
    par_edu = case_when( #<<
      # If father's edu is missing but not mother's, take mother's #<<
      (eiscedf == 55 | is.na(eiscedf)) & eiscedm != 55 & !is.na(eiscedm) ~ eiscedm, #<<
      # If mother's edu is missing but not father's, take father's #<<
      (eiscedm == 55 | is.na(eiscedm)) & eiscedf != 55 & !is.na(eiscedf)  ~ eiscedf, #<<
      # If both are missing, its missing #<<
      eiscedf == 55 & eiscedm == 55 ~ as.numeric(NA), #<<
      # For all others, take the average of both parents #<<
      TRUE ~ (eiscedm + eiscedf) / 2), #<<
    eduyrs = case_when( # Education
      eduyrs > 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs < 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %>% # Make it numeric, then
  dplyr::filter(
    # Keep only respondents with native-born parents,
    facntr == "Yes" & mocntr == "Yes")
```

---
# Design matrix

```{r}
# Design matrix (i.e., tibble of the vars going into out model)
ESS <- ESS %>%
  select(cntry, imwbcnt, eduyrs, par_edu, gndr, pspwght) %>%
  drop_na() # Casewise deletion.
```

---
# Modeling xenophobia

.push-left[
.panelset[
.panel[.panel-name[Plot]
```{r echo = FALSE, out.width='100%', fig.height = 4, fig.width = 6}
# Plot the distribution of imwbcnt
ggplot(data = ESS, aes(x = imwbcnt)) +
  geom_histogram() +
  theme_minimal()
```
.panel[.panel-name[Code]
```{r fig.show = 'hide'}
# Plot the distribution of imwbcnt
ggplot(data = ESS, aes(x = imwbcnt)) +
  geom_freqpoly() +
  theme_minimal()
```
]
]]]

--

.push-right[
#### Modeling
Coming up with a model of imwbcntphobia means: Can imwbcntphobia (i.e., $y$) be expressed as a function (i.e., $f()$) of another variable (i.e., $x$)?

$$\hat{y} = f(x)$$
The two critical questions are thus:

1. What is $f()$?
2. What is $x$? <br> .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]
]

---
# A linear Model of $y$ .font60[Simple, but extremely versatile!]

.right-column[
$$\hat{y} = \alpha + \beta x$$

```{r, echo = FALSE, out.width='100%'}
knitr::include_graphics('./img/LinearModel.png')
```
]

--

.left-column[
Now, the two critical questions have turned to:
1. What are $\alpha$ and $\beta$?
2. What is $x$? <br> .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]

]

---
# A linear Model of $y$

.left-column[
Let's try $x = \text{years of education}$, since schools in most countries teach humanism and tolerance: Is imwbcntphobia a linear (i.e., constant slope) function of one's years of education?
]

.right-column[
.panelset[
.panel[.panel-name[Plot]
```{r echo = FALSE, out.width='85%', fig.height = 4, fig.width = 7}
ggplot(data = ESS, mapping = aes(y = imwbcnt, x = eduyrs)) +
  geom_jitter(aes(size = pspwght), alpha = 1/5) + 
  geom_smooth(aes(weight = pspwght), se = FALSE) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE, color = "red") +
  labs(y = "Xenophobia", x =  "Years of education", size = "Survey \n weight") +
  theme_minimal()
```
]
.panel[.panel-name[Code]
```{r fig.show = "hide"}
ggplot(data = ESS, mapping = aes(y = imwbcnt, x = eduyrs)) +
  geom_jitter(aes(size = pspwght), alpha = 1/5) + 
  geom_smooth(aes(weight = pspwght), se = FALSE) +
  geom_smooth(aes(weight = pspwght), method = "lm", 
              se = FALSE, color = "red") +
  labs(y = "Xenophobia", x =  "Years of education", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Ordinary Least Squares

.left-column[
We find $\alpha$ and $\beta$ by choosing the one line that minimizes the *residual sum of squares*:

$$\begin{align*}
      \min \text{RSS} &= \min \sum_{i=1}^{n} e_{i}^{2} \\
      &= \min \sum_{i=1}^{n} y_{i} - \hat{y_{i}} \\
      &= \min \sum_{i=1}^{n} (y_{i} - (\alpha + \beta x_{i})^{2}
    \end{align*}$$
    
.backgrnote[
Note, how the residuals are defined only in terms of $y$ and thereby differ fundamentally from those that PCA minimizes!
]
]

.right-column[
```{r OLS, echo = FALSE, out.width='90%', fig.height = 4, fig.width = 6}
library(modelr)
mod <- lm(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
eduyrs <- rnorm(mean = 15, sd = 3, n = 75)
imwbcnt <-  coef(mod)[1] + coef(mod)[2]*eduyrs + rnorm(mean = 0, sd = 0.5, n = 75)  
dat <- tibble(imwbcnt, eduyrs)
mod2 <- lm(formula = imwbcnt ~ eduyrs, data = dat)

dat <- dat %>% add_residuals(model = mod2) %>% add_predictions(model = mod2)

ggplot(data = dat, mapping = aes(y = imwbcnt, x = eduyrs)) +
  geom_point() +
  geom_linerange(mapping = aes(ymin = pred, ymax = (resid + pred)), color = "#901A1E") +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Xenophobia", x =  "Years of education") +
  theme_minimal()
```

]

---
# The `lm()` function 

.right-column[
```{r}
# Estimate a linear OLS model and assign the results to object ols.
ols <- lm(formula = imwbcnt ~ eduyrs, data = ESS)
summary(ols) # Give a summary of the main results.
```
]

.left-column[

`r extract_eq(ols, use_coefs = TRUE)`

is the line that $\min \sum_{i=1}^{n} e_{i}^{2}$.

]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use (post-stratification) weights. 

.alert[But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values, and confidence intervals) will be wrong, because weights introduce heteroscedasticity.]
]

.right-column[
```{r}
# Estimate a weighted linear OLS model.
ols <- lm(formula = imwbcnt ~ eduyrs, data = ESS, 
          weights = pspwght) #<<
summary(ols) # Give a summary of the main results.
```
]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use post-stratification weights.

But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values) will be wrong, because weights introduce heteroscedasticity.

.alert[We thus need to estimate heteroscedasticity-robust standard errors.] One comfortable possibility to do that is `estimatr::lm_robust()`.
]

.right-column[
```{r}
# Estimate a weighted linear OLS model.
ols <- lm_robust(formula = imwbcnt ~ eduyrs, data = ESS, #<<
                 weights = pspwght)
summary(ols) # Give a summary of the main results.
```
]

---
# Multiple linear OLS

.left-column[
```{r echo = FALSE, out.width='100%', fig.height = 4, fig.width = 5}
dagify( # Define the DAG
  # Define the theoretical causal model
  imwbcnt ~ Edu + Par_edu + Conf,
  Edu ~ Par_edu + Conf,
  # Add labels
  labels = c("imwbcnt" = "Xenophobia", 
             "Edu" = "Years of Education",
             "Par_edu" = "Parental education"),
  # Specify coordinates for the graph
  coords = list(
    x = c(Edu = 0, Par_edu = 0, imwbcnt = 1),
    y = c(Edu = 0, Par_edu = 1, imwbcnt = 0.5))
) %>%
  ggdag(text = FALSE, use_labels = "label") + 
  theme_dag_blank()
```

.alert[But is parental education the only confounder?] 

.backgrnote[
A confounder is a common cause of both one's predictor $x_{i1}$ (e.g., years of education) and of the outcome $y_i$ (e.g., imwbcntphobia). Counfounder bias is: $\mathbb{E}(\tilde{\beta_{x_{1}}}) = \beta_{x_{1}} + \color{orange}{\beta_{x_{2}} \frac{\text{Cov}(x_{i1},x_{i2})}{\text{Var}(x_{i1})}}.$
]
]

.right-column[
In the social sciences we use OLS to test theories about causal effects. It is thus important to rule out alternative explanations.
```{r}
# Estimate a weighted linear OLS model.
ols2 <- lm_robust(
  formula = imwbcnt ~ eduyrs + par_edu, #<<
  data = ESS, weights = pspwght)
summary(ols2) # Give a summary of the main results.
```
]

---
# Categorical predictors

.left-column[
.center[.content-box-blue[
How can we make sense of a categorical predictor in a linear model?]]
]

--

.right-column[
.panelset[
.panel[.panel-name[Plot]
```{r dummies, out.width='90%', fig.height = 4, fig.width = 6, echo = FALSE}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = case_when(
                       cntry == "Denmark" ~ 0,
                       cntry == "Sweden" ~ 1,
                       TRUE ~ as.numeric(NA)
                     ))) +
  geom_jitter(aes(size = pspwght), alpha = 1/15, 
              width = 0.05, height = 0) +
  geom_smooth(aes(weight = pspwght), 
              method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(0, 1), 
                     labels = c("0 = Denmark", 
                                "1 = Sweden")) +
  labs(x = "Country", y = "Xenophobia", 
       size = "Survey \n weight") +
  theme_minimal()
```
]
.panel[.panel-name[Code]
```{r fig.show = "hide"}
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = case_when(
                       cntry == "Denmark" ~ 0,
                       cntry == "Sweden" ~ 1,
                       TRUE ~ as.numeric(NA)
                     ))) +
  geom_jitter(aes(size = pspwght), alpha = 1/15, width = 0.05, height = 0) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(0, 1), 
                     labels = c("0 = Denmark", "1 = Sweden")) +
  labs(x = "Country", y = "Xenophobia", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Categorical predictors

.left-column[
Factors are automatically treated as dummies, with the first level taken as reference. Thus if you want to change the reference, you need to recode the levels of the factor using the `forcats` package.
]

.right-column[
.font90[
```{r}
ols3 <- lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, 
                  data = ESS, weights = pspwght)
summary(ols3) # Give a summary of the main results.
```
]]

---
# Regression tables

.font90[
```{r warning = FALSE, message = FALSE}
# Regression table of model objects ols1 to ols3; report standard errors, not confidence intervals.
texreg::screenreg(list(ols, ols2, ols3), include.ci = FALSE) 
```
]

---
# Regression tables

.push-left[
```{r}
# Regression table of model objects ols1 to ols3; 
# report standard errors, not confidence intervals.
texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.doc") # Word table

texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.html") # Html table

texreg::texreg(list(ols, ols2, ols3), 
       include.ci = FALSE, 
       file =  "MyOLSModels.tex") # Latex table 
```

```{r eval = FALSE}
# Regression table with labelled predicators
htmlreg(list(ols, ols2, ols3), include.ci = FALSE,
        custom.coef.names = c("Intercept", 
                              "Years of Education", 
                              "Age", "Women", 
                              "Denmark", "Norway", "Sweden"),
        caption = "My regression table", 
        caption.above = TRUE,
        single.row = TRUE)
```
]

.push-right[
.font60[
```{r results = 'asis', echo = FALSE}
htmlreg(list(ols, ols2, ols3), include.ci = FALSE, doctype = FALSE,
        custom.coef.names = c("Intercept", "Years of Education", "Age", "Women", "Denmark", "Norway", "Sweden"), 
        caption = "My regression table", 
        caption.above = TRUE,
        single.row = TRUE) # Html table
```
]]

---
# Further processing of model results

`broom:tidy()` turns a model object into a tibble containing coefficients and inference stats.
```{r}
tidy(ols3) 
```

---
# Regression results are data too!

```{r out.width='50%', fig.align='center', dpi = 350, fig.height = 3, fig.width = 5}
(plotdata <- ols3 %>% tidy() %>% # Use the ols5 model object, then tidy it, then
   mutate(term = fct_recode(factor(term), # Recode predictor names, then
                            "Years of Education" = "eduyrs",
                            "Parental education" = "par_edu",
                            "Women" = "gndrFemale",
                            "Intercept" = "(Intercept)",
                            "Denmark" = "cntryDenmark",
                            "Norway" = "cntryNorway",
                            "Sweden" = "cntrySweden"),
          predictor = case_when( # Identify explanatry and confounding variables
            term == "Years of Education" ~ "Predictor",
            TRUE ~ "Potential confounder"))) %>% 
  select(term, estimate, conf.low, conf.high, predictor) # keep only some of all the info.
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
```{r out.width='60%', fig.height = 4, fig.width = 6, echo = FALSE}
ggplot(data = plotdata,
       aes(x = reorder(term, estimate), 
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange() + # Coefs with 95% CI.
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank())
```
]
.panel[.panel-name[Code]
```{r fig.show = "hide"}
ggplot(data = plotdata,
       aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high, color = predictor)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange() + # Coefs with 95% CI.
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank())
```
]]

---
class: clear
# Further processing of model results .font60[Several models]
```{r}
(plotdata <- bind_rows( # Stack several tibbles on top of each other into one tibble, #<<
  tidy(ols), tidy(ols2), # turn the two models into tibbles, #<<
  .id = "model" # Identify which model the tibbles came from. #<<
)) 
```

---
# Further processing of model results

```{r}
(plotdata <- bind_rows(tidy(ols), tidy(ols2), # Stack the data of two model objects into one tibble.
                       .id = "model") %>% # Identify which model the data came from, then
   # Get rid of the intercept, because it is so large compared to the other coefficients.
   dplyr::filter(term != "(Intercept)") %>%
   mutate(
     model = case_when(model == 1 ~ "Model 1", # Rename model identifyer.
                       model == 2 ~ "Model 2"),
     term = fct_recode(factor(term), # Recode predictor names, then
                       "Years of Education" = "eduyrs",
                       "Parental education" = "par_edu"),
     predictor = case_when( # Identify explanatry and confounding variables
       term == "Years of Education" ~ "Predictor",
       TRUE ~ "Potential confounder")) %>% 
   select(term, estimate, conf.low, conf.high, model, predictor)) # keep only some of all the info.
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
```{r out.width='60%', fig.height = 4, fig.width = 6, echo = FALSE}
ggplot(data = plotdata, 
       aes(x = reorder(term, estimate),
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor, shape = model)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange(# Coefs & 95% CI. #<<
    position = position_dodge(width=0.5)) + #<<
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank(),
        legend.box = 'vertical')
```
]
.panel[.panel-name[Code]
```{r coefplot2, fig.show = "hide"}
ggplot(data = plotdata, 
       aes(x = reorder(term, estimate),
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor, shape = model)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange(# Coefs & 95% CI. #<<
    position = position_dodge(width=0.5)) + #<<
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank(),
        legend.box = 'vertical')
```
]]

---
class: inverse
# General lessons

1. Again: everything in R is an object and you can always further process it. For instance, regression results are also just data, which you can visualize, table, join to other data, etc.

---
class: inverse
# Important functions

1. `lm()`: Estimate linear OLS regression. `estimatr::lm_robust()` estimates OLS regression with heteroscedasticity-robust standard errors (or cluster-robust standard errors if you wish).
2. `broom::tidy()`: Return tibble of model results.
3. `textreg::texreg()`, `textreg::htmlreg()`, and `textreg::screenreg()`: Create nicely-formatted (html, Word, ASCII, or Latex) tables of (one or several) regression models.
4. `dplyr::bind_rows()`: Add the rows of a data frames to another data frame that has equal columns/variables.
5. `dplyr::bind_cols()`: Add rows of another data frame to a data frame that has an equal number of rows. In contrast to a join/merge, no identifying key is specified. This can be helpful, but is also a bit dangerous!
6. `scale()` z-standardizes variables. But sometimes it returns a matrix rather than a vector. Therefore it makes sense to always code `scale(x) %>% as.numeric()` to ensure you get an numeric vector out of it.
