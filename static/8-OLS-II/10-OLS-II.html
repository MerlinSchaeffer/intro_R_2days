<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Intro to R for Social Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Merlin Schaeffer  Department of Sociology" />
    <meta name="date" content="2021-03-08" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="libs/Merlin169.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to R for Social Data Science
## Linear OLS Regression II
### Merlin Schaeffer<br> Department of Sociology
### 2021-03-08

---


# Let's get ready


```r
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(essurvey) # Add ESS API package.
library(ggplot2) # Allows us to create nice figures.
library(factoextra) # Allows us to visualize PCAs.
library(psych) # Supports PCA &amp; factor analysis with several useful functions.
library(estimatr) # Allows us to estimate (cluster-)robust standard errors.
library(texreg) # Allows us to make nicely-formatted Html &amp; Latex regression tables.
library(broom) # Allows us to turn parts of model objects into tibbles.
*library(modelr) # Model predictions (can also do resampling).
```



---
class: clear


```r
ESS &lt;- ESS %&gt;% transmute( # Create new variables and keep only those
*   cntry = as_factor(cntry) %&gt;% fct_relevel("Denmark"), # Country of interview
    gndr = as_factor(gndr), # Gender
*   agea = zap_labels(agea), # Age
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imbgeco = max(imbgeco, na.rm = TRUE) - zap_labels(imbgeco),
    imueclt = max(imueclt, na.rm = TRUE) - zap_labels(imueclt),
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = zap_labels(pspwght),
    # Parental education (ISCED)
    eiscedf = zap_labels(eiscedf),
    eiscedm = zap_labels(eiscedm),
    par_edu = case_when(
      # If father's edu is missing but not mother's, take mother's
      (eiscedf == 55 | is.na(eiscedf)) &amp; eiscedm != 55 &amp; !is.na(eiscedm) ~ eiscedm,
      # If mother's edu is missing but not father's, take father's
      (eiscedm == 55 | is.na(eiscedm)) &amp; eiscedf != 55 &amp; !is.na(eiscedf)  ~ eiscedf,
      # If both are missing, its missing
      eiscedf == 55 &amp; eiscedm == 55 ~ as.numeric(NA),
      # For all others, take the average of both parents
      TRUE ~ (eiscedm + eiscedf) / 2),
    eduyrs = case_when( # Education
      eduyrs &gt; 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs &lt; 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %&gt;% # Make it numeric, then
  dplyr::filter(
    facntr == "Yes" &amp; mocntr == "Yes" &amp;
      (cntry == "Denmark" | cntry == "Sweden" | cntry == "Bulgaria" | cntry == "Germany"))
```

---
# PCA &amp; design matrix


```r
# Perform pca of three items and write the results into object xeno.
(xeno &lt;- ESS %&gt;% 
   prcomp(formula = ~ imbgeco + imueclt + imwbcnt, 
          data = ., scale = TRUE, center = TRUE,
          na.action = na.exclude))
# Standard deviations (1, .., p=3):
# [1] 1.576 0.566 0.443
# 
# Rotation (n x k) = (3 x 3):
#            PC1    PC2     PC3
# imbgeco -0.560 -0.828  0.0209
# imueclt -0.585  0.414  0.6977
# imwbcnt -0.586  0.379 -0.7161

# Make a new "xeno" variable and fill it with z-standardized PC1.
ESS$xeno &lt;- -xeno$x[ , "PC1"] %&gt;% # Take PC1 scores, then
  scale() %&gt;% # z-standardize (but sometimes returns a "matrix"), then 
  as.numeric() # turn into numeric vector.

# Design matrix (i.e., tibble of the vars going into out model)
ESS &lt;- ESS %&gt;%
  select(cntry, xeno, eduyrs, par_edu, gndr, agea, pspwght) %&gt;%
  mutate(cntry = fct_drop(cntry)) %&gt;% # Drop unused cntry factor levels, then
  drop_na() # Casewise deletion.
```

---
# Morning `\(\rightarrow\)` Afternoon

.left-column[
**Morning**: OLS allows us to express a continuous outcome as *linear* function of continuous or categorical predictors. We can adjust/control for multiple confounding variables. The estimated linear association is *not conditional* on the level of the control/confounding variables.

**Afternoon**: Conditional and non-linear relationships!
]

.right-column[
.font80[

```r
# Run a series of three post-stratification weighted linear OLS models.
ols1 &lt;- lm_robust(formula = xeno ~ eduyrs, data = ESS, weights = pspwght)
ols2 &lt;- lm_robust(formula = xeno ~ eduyrs + par_edu, data = ESS, weights = pspwght)
ols3 &lt;- lm_robust(formula = xeno ~ eduyrs + par_edu + gndr + cntry, data = ESS, weights = pspwght)
# Report results in a regression table.
screenreg(list(ols1, ols2, ols3), include.ci = FALSE) 
# 
# ====================================================
#                Model 1      Model 2      Model 3    
# ----------------------------------------------------
# (Intercept)       0.84 ***     0.90 ***     0.72 ***
#                  (0.06)       (0.06)       (0.07)   
# eduyrs           -0.06 ***    -0.05 ***    -0.04 ***
#                  (0.00)       (0.00)       (0.00)   
# par_edu                       -0.06 ***    -0.06 ***
#                               (0.01)       (0.01)   
# gndrFemale                                 -0.08 ** 
#                                            (0.03)   
# cntryBulgaria                               0.54 ***
#                                            (0.04)   
# cntryGermany                               -0.09 *  
#                                            (0.04)   
# cntrySweden                                -0.39 ***
#                                            (0.04)   
# ----------------------------------------------------
# R^2               0.04         0.05         0.16    
# Adj. R^2          0.04         0.05         0.16    
# Num. obs.      5681         5681         5681       
# RMSE              0.96         0.96         0.90    
# ====================================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]]

---
# Conditional relations .font60[Aka interaction effects]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-6-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, 
       mapping = aes(y = xeno, x = eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  labs(x = "Years of education", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]]

---
class: clear
# Interaction effects .font60[Specify them as what they are: *multiplicative* terms]

.right-column[
.font70[

```r
ols4 &lt;- lm_robust(formula = xeno ~ par_edu + gndr +
*                   eduyrs*cntry,
                  data = ESS, weights = pspwght)
screenreg(ols4, include.ci = FALSE) 
# 
# =================================
#                       Model 1    
# ---------------------------------
# (Intercept)              0.92 ***
#                         (0.12)   
# par_edu                 -0.06 ***
#                         (0.01)   
# gndrFemale              -0.08 ** 
#                         (0.03)   
# eduyrs                  -0.05 ***
#                         (0.01)   
# cntryBulgaria           -0.43 *  
#                         (0.17)   
# cntryGermany             0.04    
#                         (0.16)   
# cntrySweden             -0.08    
#                         (0.18)   
# eduyrs:cntryBulgaria     0.08 ***
#                         (0.01)   
# eduyrs:cntryGermany     -0.01    
#                         (0.01)   
# eduyrs:cntrySweden      -0.02    
#                         (0.01)   
# ---------------------------------
# R^2                      0.17    
# Adj. R^2                 0.17    
# Num. obs.             5681       
# RMSE                     0.90    
# =================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]]

--

.left-column[
How do we interpret the estimates correctly?

Why is `\(\beta_{\text{eduyrs}}\)` smaller than in model 3?

Why do Bulgarians suddenly seem to have *lower* levels of xenophobia, whereas before they had higher ones?
]

---
# Interaction effects .font60[Beware of the scaling of the Xs!]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-9-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, 
       mapping = aes(y = xeno, x = eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE,
*             # draw line beyond observed cases.
*             fullrange = TRUE) +
* xlim(c(0, 21)) + # Extend range of x-axis.
  labs(x = "Years of education", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]]

---
# Solution: Rescale .font60[(e.g., z-standardization)]

.panelset[
.panel[.panel-name[Code]

```r
ESS &lt;- ESS %&gt;% mutate( 
  # create z-standardized version of continuous predictor.
  z_eduyrs = scale(eduyrs) %&gt;% as.numeric()
)
```


```r
ggplot(data = ESS, 
       mapping = aes(y = xeno, x = z_eduyrs, color = cntry)) +
  geom_jitter(aes(size = pspwght), alpha = 1/7) +
  geom_vline(xintercept = 0, color = "#901A1E") + 
  geom_smooth(aes(weight = pspwght), 
              method = "lm", se = FALSE) +
  labs(x = "Years of education \n (z-standardized)", y = "Xenophobia", size = "Survey \n weight", color = "Country") +
  theme_minimal()
```
]
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-12-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]]

---
# Interaction effects .font60[Improved interpretability]

.push-right[
.font70[

```r
ols4 &lt;- lm_robust(formula = xeno ~ par_edu + gndr +
                    z_eduyrs*cntry,
                  data = ESS, weights = pspwght)
screenreg(ols4, include.ci = FALSE) 
# 
# ===================================
#                         Model 1    
# -----------------------------------
# (Intercept)                0.21 ***
#                           (0.04)   
# par_edu                   -0.06 ***
#                           (0.01)   
# gndrFemale                -0.08 ** 
#                           (0.03)   
# z_eduyrs                  -0.17 ***
#                           (0.03)   
# cntryBulgaria              0.60 ***
#                           (0.04)   
# cntryGermany              -0.08 *  
#                           (0.04)   
# cntrySweden               -0.39 ***
#                           (0.04)   
# z_eduyrs:cntryBulgaria     0.25 ***
#                           (0.04)   
# z_eduyrs:cntryGermany     -0.03    
#                           (0.04)   
# z_eduyrs:cntrySweden      -0.07    
#                           (0.04)   
# -----------------------------------
# R^2                        0.17    
# Adj. R^2                   0.17    
# Num. obs.               5681       
# RMSE                       0.90    
# ===================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]]

.push-left[
How do we interpret the estimates correctly?
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Non-linearities? .font60[Transform using `I()` and `poly()`]

.push-left[
`I()` allows you to perform any transformation without a need to change the underlying tibble/design matrix.

`poly()` introduces *orthogonal* polynomials (i.e., squared, and higher degrees). If you don't want them to be orthogonal, you can use the option `raw = TRUE`.

.alert[Beware of over-fitting!!]

```r
ols6 &lt;- lm_robust(
  formula = xeno ~ gndr + cntry + eduyrs +
*   poly(agea, 2, raw = TRUE) +
*   I(log(par_edu)),
  data = ESS, weights = pspwght)
```
]

.push-right[
.font80[

```r
screenreg(ols6, include.ci = FALSE, digits = 4) 
# 
# =========================================
#                             Model 1      
# -----------------------------------------
# (Intercept)                    0.3351 ** 
#                               (0.1064)   
# gndrFemale                    -0.0858 ** 
#                               (0.0279)   
# cntryBulgaria                  0.5232 ***
#                               (0.0427)   
# cntryGermany                  -0.1002 ** 
#                               (0.0372)   
# cntrySweden                   -0.4033 ***
#                               (0.0406)   
# eduyrs                        -0.0454 ***
#                               (0.0047)   
# poly(agea, 2, raw = TRUE)1     0.0135 ** 
#                               (0.0041)   
# poly(agea, 2, raw = TRUE)2    -0.0001 *  
#                               (0.0000)   
# log(par_edu)                  -0.0754 *  
#                               (0.0308)   
# -----------------------------------------
# R^2                            0.1592    
# Adj. R^2                       0.1580    
# Num. obs.                   5681         
# RMSE                           0.9026    
# =========================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]]

---
# Model visualization .font60[Especially for lay audiences!]

.push-left[
**Step 1**: Generate a synthetic tibble with theoretically-informative values containing all variables of the model you want to visualize. `modelr::data_grid()` generates a tibble with all combinations of the variables you feed to it.
.font80[

```r
(synth_data &lt;- ESS %&gt;%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
*  data_grid(eduyrs, par_edu, gndr, agea, cntry))
# # A tibble: 154,128 x 5
#    eduyrs par_edu gndr   agea cntry   
#     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;   
#  1      9       1 Male     15 Denmark 
#  2      9       1 Male     15 Bulgaria
#  3      9       1 Male     15 Germany 
#  4      9       1 Male     15 Sweden  
#  5      9       1 Male     16 Denmark 
#  6      9       1 Male     16 Bulgaria
#  7      9       1 Male     16 Germany 
#  8      9       1 Male     16 Sweden  
#  9      9       1 Male     17 Denmark 
# 10      9       1 Male     17 Bulgaria
# # … with 154,118 more rows
```
]]

--

.push-right[
.font80[
**Step 2**: Alter your new tibble, so that only one predictor of interest varies. Set all others to an informative constant value.

```r
(synth_data &lt;- synth_data %&gt;%
   mutate( # Set variables constant that you don't want to vidualize,
     eduyrs = mean(ESS$eduyrs, na.rm = TRUE),
     par_edu = mean(ESS$par_edu, na.rm = TRUE),
     gndr = "Female",
     cntry = "Denmark") %&gt;% # then
   unique()) # Drop duplicates.
# # A tibble: 76 x 5
#    eduyrs par_edu gndr    agea cntry  
#     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;  
#  1   13.8    3.14 Female    15 Denmark
#  2   13.8    3.14 Female    16 Denmark
#  3   13.8    3.14 Female    17 Denmark
#  4   13.8    3.14 Female    18 Denmark
#  5   13.8    3.14 Female    19 Denmark
#  6   13.8    3.14 Female    20 Denmark
#  7   13.8    3.14 Female    21 Denmark
#  8   13.8    3.14 Female    22 Denmark
#  9   13.8    3.14 Female    23 Denmark
# 10   13.8    3.14 Female    24 Denmark
# # … with 66 more rows
```
]]

---
# Model predictions

.push-left[
**Step 3**: Add predictions, that is, apply the estimated model to the synthetic data. Given the synthetic data, what `\(\hat{y}\)` does your model predict?

```r
# Generate predictions and 95% CI, then
*(synth_data &lt;- predict(ols6, newdata = synth_data,
*                      interval = "confidence",
*                      level = 0.95)$fit %&gt;%
   as_tibble() %&gt;% # Turn into a tibble, then
   # Add to the synthetic data.
   bind_cols(synth_data, .)) 
```
]

.push-right[
.font80[

```
# # A tibble: 76 x 8
#    eduyrs par_edu gndr    agea cntry      fit    lwr    upr
#     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
#  1   13.8    3.14 Female    15 Denmark -0.282 -0.384 -0.179
#  2   13.8    3.14 Female    16 Denmark -0.272 -0.370 -0.173
#  3   13.8    3.14 Female    17 Denmark -0.261 -0.356 -0.167
#  4   13.8    3.14 Female    18 Denmark -0.252 -0.342 -0.161
#  5   13.8    3.14 Female    19 Denmark -0.242 -0.329 -0.154
#  6   13.8    3.14 Female    20 Denmark -0.232 -0.316 -0.148
#  7   13.8    3.14 Female    21 Denmark -0.223 -0.304 -0.142
#  8   13.8    3.14 Female    22 Denmark -0.214 -0.293 -0.135
#  9   13.8    3.14 Female    23 Denmark -0.205 -0.281 -0.129
# 10   13.8    3.14 Female    24 Denmark -0.196 -0.271 -0.122
# # … with 66 more rows
```
]]

---
# Visualize model predictions

.push-left[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-20-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = synth_data, 
       aes(y = fit, x = agea)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              alpha = 0.5) +
  geom_line() +
  labs(x = "Age",
  y = "Xenophobia",
  caption = "(Covariates set to: Women in Denmark \n at mean levels of (parental) education)") + 
  theme_minimal()
```
]]]
.push-right[
.font80[

```r
screenreg(ols6, include.ci = FALSE, digits = 4) 
# 
# =========================================
#                             Model 1      
# -----------------------------------------
# (Intercept)                    0.3351 ** 
#                               (0.1064)   
# gndrFemale                    -0.0858 ** 
#                               (0.0279)   
# cntryBulgaria                  0.5232 ***
#                               (0.0427)   
# cntryGermany                  -0.1002 ** 
#                               (0.0372)   
# cntrySweden                   -0.4033 ***
#                               (0.0406)   
# eduyrs                        -0.0454 ***
#                               (0.0047)   
# poly(agea, 2, raw = TRUE)1     0.0135 ** 
#                               (0.0041)   
# poly(agea, 2, raw = TRUE)2    -0.0001 *  
#                               (0.0000)   
# log(par_edu)                  -0.0754 *  
#                               (0.0308)   
# -----------------------------------------
# R^2                            0.1592    
# Adj. R^2                       0.1580    
# Num. obs.                   5681         
# RMSE                           0.9026    
# =========================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]]

---
# One more time ...

.push-left[
.font90[

```r
# Generate synthetic data to visualize parental education
synth_data &lt;- ESS %&gt;%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
   data_grid(eduyrs, par_edu, gndr, agea, cntry) %&gt;%
   mutate( # Set confounders constant at their mean,
     eduyrs = mean(ESS$eduyrs, na.rm = TRUE),
     agea = mean(ESS$agea, na.rm = TRUE),
     gndr = "Female",
     cntry = "Denmark") %&gt;% # then
   unique() # Drop duplicates.

# Generate predictions and 95% confidence intervals, then
synth_data &lt;- predict(ols6, newdata = synth_data,
                      interval = "confidence",
                      level=0.95)$fit %&gt;%
  as_tibble() %&gt;% # Turn into a tibble, then
  bind_cols(synth_data, .) # Add to the synthetic data.
```
]]

.push-right[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-24-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]
.font90[

```r
ggplot(data = synth_data, aes(y = fit, x = par_edu)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.5) +
  geom_line() +
  labs(x = "Parental education", y = "Xenophobia", 
       caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]]]]
---
# Visualizing interactions

.push-left[
.font90[

```r
# Estimate linear OLS with interaction effect;
# no z-standardization if you visualize.
ols7 &lt;- lm_robust(formula = xeno ~ log(par_edu) + gndr + poly(agea, 2) +
*                   eduyrs*cntry,
                  data = ESS, weights = pspwght)

# Generate synthetic data to 
# visualize parental education
synth_data &lt;- ESS %&gt;%
   # Generate theoretically-informative 
   # but artificial tibble for predictions.
   data_grid(par_edu, gndr, cntry, eduyrs, agea) %&gt;%
   mutate( # Set confounders constant at their mean,
*    # but let both cntry and eduyrs vary.
     par_edu = mean(ESS$par_edu, na.rm = TRUE),
     agea = mean(ESS$agea, na.rm = TRUE),
     gndr = "Female") %&gt;% # then
   unique() # Drop duplicates.

# Generate predictions and 95% confidence intervals, then
synth_data &lt;- predict(ols7, newdata = synth_data,
                      interval = "confidence",
                      level=0.95)$fit %&gt;%
  as_tibble() %&gt;% # Turn into a tibble, then
  bind_cols(synth_data, .) # Add to the synthetic data.
```
]]

.push-right[
.font80[

```
# # A tibble: 52 x 8
#    par_edu gndr   cntry    eduyrs  agea      fit     lwr     upr
#      &lt;dbl&gt; &lt;chr&gt;  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#  1    3.14 Female Denmark       9  52.3  0.238    0.122   0.353 
#  2    3.14 Female Denmark      10  52.3  0.180    0.0769  0.283 
#  3    3.14 Female Denmark      11  52.3  0.122    0.0305  0.215 
#  4    3.14 Female Denmark      12  52.3  0.0649  -0.0175  0.147 
#  5    3.14 Female Denmark      13  52.3  0.00723 -0.0678  0.0823
#  6    3.14 Female Denmark      14  52.3 -0.0504  -0.121   0.0202
#  7    3.14 Female Denmark      15  52.3 -0.108   -0.178  -0.0384
#  8    3.14 Female Denmark      16  52.3 -0.166   -0.238  -0.0933
#  9    3.14 Female Denmark      17  52.3 -0.223   -0.302  -0.145 
# 10    3.14 Female Denmark      18  52.3 -0.281   -0.368  -0.194 
# 11    3.14 Female Denmark      19  52.3 -0.339   -0.436  -0.241 
# 12    3.14 Female Denmark      20  52.3 -0.396   -0.505  -0.287 
# 13    3.14 Female Denmark      21  52.3 -0.454   -0.576  -0.332 
# 14    3.14 Female Bulgaria      9  52.3  0.475    0.372   0.577 
# 15    3.14 Female Bulgaria     10  52.3  0.490    0.400   0.580 
# 16    3.14 Female Bulgaria     11  52.3  0.505    0.426   0.584 
# 17    3.14 Female Bulgaria     12  52.3  0.520    0.449   0.591 
# 18    3.14 Female Bulgaria     13  52.3  0.535    0.468   0.603 
# 19    3.14 Female Bulgaria     14  52.3  0.550    0.481   0.619 
# 20    3.14 Female Bulgaria     15  52.3  0.565    0.491   0.640 
# 21    3.14 Female Bulgaria     16  52.3  0.580    0.497   0.664 
# 22    3.14 Female Bulgaria     17  52.3  0.596    0.500   0.691 
# 23    3.14 Female Bulgaria     18  52.3  0.611    0.501   0.720 
# 24    3.14 Female Bulgaria     19  52.3  0.626    0.502   0.750 
# 25    3.14 Female Bulgaria     20  52.3  0.641    0.501   0.781 
# 26    3.14 Female Bulgaria     21  52.3  0.656    0.500   0.812 
# 27    3.14 Female Germany       9  52.3  0.195    0.0861  0.303 
# 28    3.14 Female Germany      10  52.3  0.128    0.0314  0.225 
# # … with 24 more rows
```
]]

---
# Visualizing interactions

.panelset[
.panel[.panel-name[Plot]
&lt;img src="10-OLS-II_files/figure-html/unnamed-chunk-28-1.png" width="65%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = synth_data, aes(y = fit, x = eduyrs)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, 
*                 fill = cntry),
              alpha = 0.5) +
* geom_line(aes(color = cntry)) +
  labs(x = "Years of education", y = "Xenophobia", 
       caption = "(Covariates set to: Women at mean levels of parental education)") + 
  theme_minimal()
```
]]

---
class: inverse
# Today's general lessons

1. Linear OLS models are very versatile and can approximate very complex relationships. Theoretically, interaction effects, transformed variables and polynomials allow us to *linearize* any relationship/functional form.
2. You can formulate conditional and non-linear relationships directly in your model's formula.
3. It is important to visualize such complex relationships, because regression coefficients alone are hard to interpret. You can do that by making predictions from your model for synthetic data.

---
class: inverse
# Today's (important) functions

1. `I()` allows you to transform predictors in the formula argument of your regression model.
2. `poly()` allows you to introduce orthogonal polynomials to your model.
3. `predic()` generates predictions from your model for a tibble that you feed to it.
4. `modelr::data_grid()` generates synthetic data sets that contain all combinations of variables you feed to it.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
