<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Intro to R for Social Data Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Merlin Schaeffer &amp; Friedolin Merhout   Department of Sociology" />
    <meta name="date" content="2021-10-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="libs/Merlin169.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to R for Social Data Science
## Linear OLS Regression I
### Merlin Schaeffer &amp; Friedolin Merhout <br> Department of Sociology
### 2021-10-18

---


# Let's get ready


```r
# Add packages to library
library(tidyverse) # Add the tidyverse package to my current library.
library(haven) # Handle labelled data.
library(essurvey) # Add ESS API package.
library(ggplot2) # Allows us to create nice figures.
library(factoextra) # Allows us to visualize PCAs.
library(psych) # Supports PCA &amp; factor analysis with several useful functions.
*library(estimatr) # Allows us to estimate (cluster-)robust standard errors.
*library(texreg) # Allows us to make nicely-formatted Html &amp; Latex regression tables.
*library(broom) # Allows us to turn model objects into tibbles.
```



---
class: clear


```r
ESS &lt;- ESS %&gt;% transmute( # Create new variables and keep only those
    cntry = as_factor(cntry), # Country of interview
    gndr = as_factor(gndr),
    facntr = as_factor(facntr), # Father born in cntry
    mocntr = as_factor(mocntr), # Mother born in cntry
    imwbcnt = max(imwbcnt, na.rm = TRUE) - zap_labels(imwbcnt),
    pspwght = zap_label(pspwght),
*   # Parental education (ISCED)
*   eiscedf = zap_labels(eiscedf),
*   eiscedm = zap_labels(eiscedm),
*   par_edu = case_when(
*     # If father's edu is missing but not mother's, take mother's
*     (eiscedf == 55 | is.na(eiscedf)) &amp; eiscedm != 55 &amp; !is.na(eiscedm) ~ eiscedm,
*     # If mother's edu is missing but not father's, take father's
*     (eiscedm == 55 | is.na(eiscedm)) &amp; eiscedf != 55 &amp; !is.na(eiscedf)  ~ eiscedf,
*     # If both are missing, its missing
*     eiscedf == 55 &amp; eiscedm == 55 ~ as.numeric(NA),
*     # For all others, take the average of both parents
*     TRUE ~ (eiscedm + eiscedf) / 2),
    eduyrs = case_when( # Education
      eduyrs &gt; 21 ~ 21, # Recode to max 21 years of edu.
      eduyrs &lt; 9 ~ 9, # Recode to min 9 years of edu.
      TRUE ~ zap_labels(eduyrs))) %&gt;% # Make it numeric, then
  dplyr::filter(
    # Keep only respondents with native-born parents,
    facntr == "Yes" &amp; mocntr == "Yes" &amp;
      (cntry == "Denmark" | cntry == "Sweden" | cntry == "Norway" | cntry == "Germany"))
```

---
# Design matrix


```r
# Design matrix (i.e., tibble of the vars going into out model)
ESS &lt;- ESS %&gt;%
  select(cntry, imwbcnt, eduyrs, par_edu, gndr, pspwght) %&gt;%
  drop_na() # Casewise deletion.
```

---
# Modeling xenophobia

.push-left[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="6-OLS-I_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
.panel[.panel-name[Code]

```r
# Plot the distribution of imwbcnt
ggplot(data = ESS, aes(x = imwbcnt)) +
  geom_freqpoly() +
  theme_minimal()
```
]
]]]

--

.push-right[
#### Modeling
Coming up with a model of imwbcntphobia means: Can imwbcntphobia (i.e., `\(y\)`) be expressed as a function (i.e., `\(f()\)`) of another variable (i.e., `\(x\)`)?

`$$\hat{y} = f(x)$$`
The two critical questions are thus:

1. What is `\(f()\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]
]

---
# A linear Model of `\(y\)` .font60[Simple, but extremely versatile!]

.right-column[
`$$\hat{y} = \alpha + \beta x$$`

&lt;img src="./img/LinearModel.png" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.left-column[
Now, the two critical questions have turned to:
1. What are `\(\alpha\)` and `\(\beta\)`?
2. What is `\(x\)`? &lt;br&gt; .backgrnote[
You will need to find the answer to the second question in your research question, theory, and hypothesis.]

]

---
# A linear Model of `\(y\)`

.left-column[
Let's try `\(x = \text{years of education}\)`, since schools in most countries teach humanism and tolerance: Is imwbcntphobia a linear (i.e., constant slope) function of one's years of education?
]

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="6-OLS-I_files/figure-html/unnamed-chunk-8-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, mapping = aes(y = imwbcnt, x = eduyrs)) +
  geom_jitter(aes(size = pspwght), alpha = 1/5) + 
  geom_smooth(aes(weight = pspwght), se = FALSE) +
  geom_smooth(aes(weight = pspwght), method = "lm", 
              se = FALSE, color = "red") +
  labs(y = "Xenophobia", x =  "Years of education", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Ordinary Least Squares

.left-column[
We find `\(\alpha\)` and `\(\beta\)` by choosing the one line that minimizes the *residual sum of squares*:

`$$\begin{align*}
      \min \text{RSS} &amp;= \min \sum_{i=1}^{n} e_{i}^{2} \\
      &amp;= \min \sum_{i=1}^{n} y_{i} - \hat{y_{i}} \\
      &amp;= \min \sum_{i=1}^{n} (y_{i} - (\alpha + \beta x_{i})^{2}
    \end{align*}$$`
    
.backgrnote[
Note, how the residuals are defined only in terms of `\(y\)` and thereby differ fundamentally from those that PCA minimizes!
]
]

.right-column[
&lt;img src="6-OLS-I_files/figure-html/OLS-1.png" width="90%" style="display: block; margin: auto;" /&gt;

]

---
# The `lm()` function 

.right-column[

```r
# Estimate a linear OLS model and assign the results to object ols.
ols &lt;- lm(formula = imwbcnt ~ eduyrs, data = ESS)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = imwbcnt ~ eduyrs, data = ESS)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -4.894 -1.527  0.106  1.209  6.576 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  5.99717    0.12627    47.5   &lt;2e-16 ***
# eduyrs      -0.12255    0.00863   -14.2   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 2.12 on 5309 degrees of freedom
# Multiple R-squared:  0.0366,	Adjusted R-squared:  0.0364 
# F-statistic:  202 on 1 and 5309 DF,  p-value: &lt;2e-16
```
]

.left-column[

$$
\operatorname{\widehat{imwbcnt}} = 6 - 0.12(\operatorname{eduyrs})
$$


is the line that `\(\min \sum_{i=1}^{n} e_{i}^{2}\)`.

]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use (post-stratification) weights. 

.alert[But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values, and confidence intervals) will be wrong, because weights introduce heteroscedasticity.]
]

.right-column[

```r
# Estimate a weighted linear OLS model.
ols &lt;- lm(formula = imwbcnt ~ eduyrs, data = ESS, 
*         weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted Residuals:
#    Min     1Q Median     3Q    Max 
# -8.625 -1.415  0.079  1.026 10.319 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)    6.104      0.128    47.9   &lt;2e-16 ***
# eduyrs        -0.128      0.009   -14.2   &lt;2e-16 ***
# ---
# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# 
# Residual standard error: 2.13 on 5309 degrees of freedom
# Multiple R-squared:  0.0366,	Adjusted R-squared:  0.0364 
# F-statistic:  202 on 1 and 5309 DF,  p-value: &lt;2e-16
```
]

---
# Weighted linear OLS

.left-column[
The `weights` argument allows you to use post-stratification weights.

But beware, `lm()` inference (e.g., standard errors, *t*-values, *p*-values) will be wrong, because weights introduce heteroscedasticity.

.alert[We thus need to estimate heteroscedasticity-robust standard errors.] One comfortable possibility to do that is `estimatr::lm_robust()`.
]

.right-column[

```r
# Estimate a weighted linear OLS model.
*ols &lt;- lm_robust(formula = imwbcnt ~ eduyrs, data = ESS,
                 weights = pspwght)
summary(ols) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)    6.104     0.1536    39.7 1.41e-302    5.803    6.406 5309
# eduyrs        -0.128     0.0102   -12.5  2.97e-35   -0.148   -0.108 5309
# 
# Multiple R-squared:  0.0366 ,	Adjusted R-squared:  0.0364 
# F-statistic:  156 on 1 and 5309 DF,  p-value: &lt;2e-16
```
]

---
# Multiple linear OLS

.left-column[
&lt;img src="6-OLS-I_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;

.alert[But is parental education the only confounder?] 

.backgrnote[
A confounder is a common cause of both one's predictor `\(x_{i1}\)` (e.g., years of education) and of the outcome `\(y_i\)` (e.g., imwbcntphobia). Counfounder bias is: `\(\mathbb{E}(\tilde{\beta_{x_{1}}}) = \beta_{x_{1}} + \color{orange}{\beta_{x_{2}} \frac{\text{Cov}(x_{i1},x_{i2})}{\text{Var}(x_{i1})}}.\)`
]
]

.right-column[
In the social sciences we use OLS to test theories about causal effects. It is thus important to rule out alternative explanations.

```r
# Estimate a weighted linear OLS model.
ols2 &lt;- lm_robust(
* formula = imwbcnt ~ eduyrs + par_edu,
  data = ESS, weights = pspwght)
summary(ols2) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs + par_edu, data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#             Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)    6.403     0.1528   41.91 0.00e+00    6.104   6.7030 5308
# eduyrs        -0.103     0.0105   -9.81 1.56e-22   -0.124  -0.0826 5308
# par_edu       -0.191     0.0221   -8.64 7.31e-18   -0.235  -0.1478 5308
# 
# Multiple R-squared:  0.0553 ,	Adjusted R-squared:  0.055 
# F-statistic:  122 on 2 and 5308 DF,  p-value: &lt;2e-16
```
]

---
# Categorical predictors

.left-column[
.center[.content-box-blue[
How can we make sense of a categorical predictor in a linear model?]]
]

--

.right-column[
.panelset[
.panel[.panel-name[Plot]
&lt;img src="6-OLS-I_files/figure-html/dummies-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = ESS, 
       mapping = aes(y = imwbcnt, 
                     x = case_when(
                       cntry == "Denmark" ~ 0,
                       cntry == "Sweden" ~ 1,
                       TRUE ~ as.numeric(NA)
                     ))) +
  geom_jitter(aes(size = pspwght), alpha = 1/15, width = 0.05, height = 0) +
  geom_smooth(aes(weight = pspwght), method = "lm", se = FALSE) +
  scale_x_continuous(breaks = c(0, 1), 
                     labels = c("0 = Denmark", "1 = Sweden")) +
  labs(x = "Country", y = "Xenophobia", 
       size = "Survey \n weight") +
  theme_minimal()
```
]]]

---
# Categorical predictors

.left-column[
Factors are automatically treated as dummies, with the first level taken as reference. Thus if you want to change the reference, you need to recode the levels of the factor using the `forcats` package.
]

.right-column[
.font90[

```r
ols3 &lt;- lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, 
                  data = ESS, weights = pspwght)
summary(ols3) # Give a summary of the main results.
# 
# Call:
# lm_robust(formula = imwbcnt ~ eduyrs + par_edu + gndr + cntry, 
#     data = ESS, weights = pspwght)
# 
# Weighted, Standard error type:  HC2 
# 
# Coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper   DF
# (Intercept)     7.071     0.1629   43.40 0.00e+00    6.752   7.3904 5304
# eduyrs         -0.108     0.0104  -10.42 3.68e-25   -0.128  -0.0877 5304
# par_edu        -0.189     0.0221   -8.57 1.38e-17   -0.232  -0.1457 5304
# gndrFemale     -0.250     0.0664   -3.76 1.69e-04   -0.380  -0.1198 5304
# cntryDenmark   -0.446     0.0910   -4.91 9.60e-07   -0.625  -0.2680 5304
# cntryNorway    -0.558     0.0866   -6.44 1.29e-10   -0.727  -0.3879 5304
# cntrySweden    -1.224     0.0937  -13.06 2.21e-38   -1.408  -1.0403 5304
# 
# Multiple R-squared:  0.0974 ,	Adjusted R-squared:  0.0964 
# F-statistic: 68.9 on 6 and 5304 DF,  p-value: &lt;2e-16
```
]]

---
# Regression tables

.font90[

```r
# Regression table of model objects ols1 to ols3; report standard errors, not confidence intervals.
texreg::screenreg(list(ols, ols2, ols3), include.ci = FALSE) 
# 
# ===================================================
#               Model 1      Model 2      Model 3    
# ---------------------------------------------------
# (Intercept)      6.10 ***     6.40 ***     7.07 ***
#                 (0.15)       (0.15)       (0.16)   
# eduyrs          -0.13 ***    -0.10 ***    -0.11 ***
#                 (0.01)       (0.01)       (0.01)   
# par_edu                      -0.19 ***    -0.19 ***
#                              (0.02)       (0.02)   
# gndrFemale                                -0.25 ***
#                                           (0.07)   
# cntryDenmark                              -0.45 ***
#                                           (0.09)   
# cntryNorway                               -0.56 ***
#                                           (0.09)   
# cntrySweden                               -1.22 ***
#                                           (0.09)   
# ---------------------------------------------------
# R^2              0.04         0.06         0.10    
# Adj. R^2         0.04         0.05         0.10    
# Num. obs.     5311         5311         5311       
# RMSE             2.13         2.11         2.06    
# ===================================================
# *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05
```
]

---
# Regression tables

.push-left[

```r
# Regression table of model objects ols1 to ols3; 
# report standard errors, not confidence intervals.
texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.doc") # Word table

texreg::htmlreg(list(ols, ols2, ols3), 
        include.ci = FALSE, 
        file = "MyOLSModels.html") # Html table

texreg::texreg(list(ols, ols2, ols3), 
       include.ci = FALSE, 
       file =  "MyOLSModels.tex") # Latex table 
```


```r
# Regression table with labelled predicators
htmlreg(list(ols, ols2, ols3), include.ci = FALSE,
        custom.coef.names = c("Intercept", 
                              "Years of Education", 
                              "Age", "Women", 
                              "Denmark", "Norway", "Sweden"),
        caption = "My regression table", 
        caption.above = TRUE,
        single.row = TRUE)
```
]

.push-right[
.font60[
&lt;table class="texreg" style="margin: 10px auto;border-collapse: collapse;border-spacing: 0px;color: #000000;border-top: 2px solid #000000;"&gt;
&lt;caption&gt;My regression table&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 1&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 2&lt;/th&gt;
&lt;th style="padding-left: 5px;padding-right: 5px;"&gt;Model 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr style="border-top: 1px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Intercept&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;6.10 (0.15)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;6.40 (0.15)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;7.07 (0.16)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Years of Education&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.13 (0.01)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.10 (0.01)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.11 (0.01)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Age&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.19 (0.02)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.19 (0.02)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Women&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.25 (0.07)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Denmark&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.45 (0.09)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Norway&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-0.56 (0.09)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Sweden&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;&amp;nbsp;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;-1.22 (0.09)&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="border-top: 1px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.04&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.06&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Adj. R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.04&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.05&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;Num. obs.&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5311&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5311&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;5311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style="border-bottom: 2px solid #000000;"&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;RMSE&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;2.13&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;2.11&lt;/td&gt;
&lt;td style="padding-left: 5px;padding-right: 5px;"&gt;2.06&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style="font-size: 0.8em;" colspan="4"&gt;&lt;sup&gt;***&lt;/sup&gt;p &amp;lt; 0.001; &lt;sup&gt;**&lt;/sup&gt;p &amp;lt; 0.01; &lt;sup&gt;*&lt;/sup&gt;p &amp;lt; 0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
]]

---
# Further processing of model results

`broom:tidy()` turns a model object into a tibble containing coefficients and inference stats.

```r
tidy(ols3) 
#           term estimate std.error statistic  p.value conf.low conf.high   df outcome
# 1  (Intercept)    7.071    0.1629     43.40 0.00e+00    6.752    7.3904 5304 imwbcnt
# 2       eduyrs   -0.108    0.0104    -10.42 3.68e-25   -0.128   -0.0877 5304 imwbcnt
# 3      par_edu   -0.189    0.0221     -8.57 1.38e-17   -0.232   -0.1457 5304 imwbcnt
# 4   gndrFemale   -0.250    0.0664     -3.76 1.69e-04   -0.380   -0.1198 5304 imwbcnt
# 5 cntryDenmark   -0.446    0.0910     -4.91 9.60e-07   -0.625   -0.2680 5304 imwbcnt
# 6  cntryNorway   -0.558    0.0866     -6.44 1.29e-10   -0.727   -0.3879 5304 imwbcnt
# 7  cntrySweden   -1.224    0.0937    -13.06 2.21e-38   -1.408   -1.0403 5304 imwbcnt
```

---
# Regression results are data too!


```r
(plotdata &lt;- ols3 %&gt;% tidy() %&gt;% # Use the ols5 model object, then tidy it, then
   mutate(term = fct_recode(factor(term), # Recode predictor names, then
                            "Years of Education" = "eduyrs",
                            "Parental education" = "par_edu",
                            "Women" = "gndrFemale",
                            "Intercept" = "(Intercept)",
                            "Denmark" = "cntryDenmark",
                            "Norway" = "cntryNorway",
                            "Sweden" = "cntrySweden"),
          predictor = case_when( # Identify explanatry and confounding variables
            term == "Years of Education" | term == "Years of education^2" ~ "Predictor",
            TRUE ~ "Potential confounder"))) %&gt;% 
  select(term, estimate, conf.low, conf.high, predictor) # keep only some of all the info.
#                 term estimate conf.low conf.high            predictor
# 1          Intercept    7.071    6.752    7.3904 Potential confounder
# 2 Years of Education   -0.108   -0.128   -0.0877            Predictor
# 3 Parental education   -0.189   -0.232   -0.1457 Potential confounder
# 4              Women   -0.250   -0.380   -0.1198 Potential confounder
# 5            Denmark   -0.446   -0.625   -0.2680 Potential confounder
# 6             Norway   -0.558   -0.727   -0.3879 Potential confounder
# 7             Sweden   -1.224   -1.408   -1.0403 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="6-OLS-I_files/figure-html/unnamed-chunk-23-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata,
       aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high, color = predictor)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
  geom_pointrange() + # Coefs with 95% CI.
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank())
```
]]

---
class: clear
# Further processing of model results .font60[Several models]

```r
*(plotdata &lt;- bind_rows( # Stack several tibbles on top of each other into one tibble,
* tidy(ols), tidy(ols2), # turn the two models into tibbles,
* .id = "model" # Identify which model the tibbles came from.
)) 
#   model        term estimate std.error statistic   p.value conf.low conf.high   df outcome
# 1     1 (Intercept)    6.104    0.1536     39.74 1.41e-302    5.803    6.4056 5309 imwbcnt
# 2     1      eduyrs   -0.128    0.0102    -12.48  2.97e-35   -0.148   -0.1078 5309 imwbcnt
# 3     2 (Intercept)    6.403    0.1528     41.91  0.00e+00    6.104    6.7030 5308 imwbcnt
# 4     2      eduyrs   -0.103    0.0105     -9.81  1.56e-22   -0.124   -0.0826 5308 imwbcnt
# 5     2     par_edu   -0.191    0.0221     -8.64  7.31e-18   -0.235   -0.1478 5308 imwbcnt
```

---
# Further processing of model results


```r
(plotdata &lt;- bind_rows(tidy(ols), tidy(ols2), # Stack the data of two model objects into one tibble.
                       .id = "model") %&gt;% # Identify which model the data came from, then
   # Get rid of the intercept, because it is so large compared to the other coefficients.
   dplyr::filter(term != "(Intercept)") %&gt;%
   mutate(
     model = case_when(model == 1 ~ "Model 1", # Rename model identifyer.
                       model == 2 ~ "Model 2"),
     term = fct_recode(factor(term), # Recode predictor names, then
                       "Years of Education" = "eduyrs",
                       "Parental education" = "par_edu"),
     predictor = case_when( # Identify explanatry and confounding variables
       term == "Years of Education" ~ "Predictor",
       TRUE ~ "Potential confounder")) %&gt;% 
   select(term, estimate, conf.low, conf.high, model, predictor)) # keep only some of all the info.
#                 term estimate conf.low conf.high   model            predictor
# 1 Years of Education   -0.128   -0.148   -0.1078 Model 1            Predictor
# 2 Years of Education   -0.103   -0.124   -0.0826 Model 2            Predictor
# 3 Parental education   -0.191   -0.235   -0.1478 Model 2 Potential confounder
```

---
class: clear
# Coefficient plots .font60[Good alternative to regression tables]

.panelset[
.panel[.panel-name[Plot]
&lt;img src="6-OLS-I_files/figure-html/unnamed-chunk-27-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Code]

```r
ggplot(data = plotdata, 
       aes(x = reorder(term, estimate),
           y = estimate, 
           ymin = conf.low, ymax = conf.high,
           color = predictor, shape = model)) +
  geom_hline(yintercept = 0, # Null-hypothesis line.
             color = "#901A1E", lty = "dashed") + 
* geom_pointrange(# Coefs &amp; 95% CI.
*   position = position_dodge(width=0.5)) +
  coord_flip() + 
  theme_minimal() +
  # Axis title: greek beta.
  labs(y = expression(beta), 
       x = "") +
  theme(legend.position="bottom",
        legend.title=element_blank(),
        legend.box = 'vertical')
```
]]

---
class: inverse
# General lessons

1. Again: everything in R is an object and you can always further process it. For instance, regression results are also just data, which you can visualize, table, join to other data, etc.

---
class: inverse
# Important functions

1. `lm()`: Estimate linear OLS regression. `estimatr::lm_robust()` estimates OLS regression with heteroscedasticity-robust standard errors (or cluster-robust standard errors if you wish).
2. `broom::tidy()`: Return tibble of model results.
3. `textreg::texreg()`, `textreg::htmlreg()`, and `textreg::screenreg()`: Create nicely-formatted (html, Word, ASCII, or Latex) tables of (one or several) regression models.
4. `dplyr::bind_rows()`: Add the rows of a data frames to another data frame that has equal columns/variables.
5. `dplyr::bind_cols()`: Add rows of another data frame to a data frame that has an equal number of rows. In contrast to a join/merge, no identifying key is specified. This can be helpful, but is also a bit dangerous!
6. `scale()` z-standardizes variables. But sometimes it returns a matrix rather than a vector. Therefore it makes sense to always code `scale(x) %&gt;% as.numeric()` to ensure you get an numeric vector out of it.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
